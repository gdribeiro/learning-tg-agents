{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in ./venvIntelliJ/lib/python3.9/site-packages (2.22.4)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (9.3.0)\r\n",
      "Requirement already satisfied: numpy in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (1.23.5)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./venvIntelliJ/lib/python3.9/site-packages (3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-agents in ./venvIntelliJ/lib/python3.9/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Requirement already satisfied: absl-py>=0.6.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.3.0)\r\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.19.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.14.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (4.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.23.0)\r\n",
      "Requirement already satisfied: protobuf>=3.11.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (3.19.6)\r\n",
      "Requirement already satisfied: gin-config>=0.4.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.5.0)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Requirement already satisfied: pygame==2.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.1.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.2.0)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Requirement already satisfied: dm-tree in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.7)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyglet in ./venvIntelliJ/lib/python3.9/site-packages (2.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venvIntelliJ/lib/python3.9/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (4.38.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.0.6)\r\n",
      "Requirement already satisfied: numpy>=1.19 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (22.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (9.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venvIntelliJ/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./venvIntelliJ/lib/python3.9/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.12.6)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.28.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.19.6)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: setuptools in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (60.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (14.0.6)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.23.5)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.51.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (4.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: packaging in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvIntelliJ/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venvIntelliJ/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.13)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venvIntelliJ/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 16:22:10.076673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Seed for PPO actor network\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# PPO Agent\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.agents.ppo import ppo_actor_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "# old agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# register(\n",
    "#     id='CartPole-v1',\n",
    "#     entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "#     max_episode_steps=500,\n",
    "#     reward_threshold=475.0,\n",
    "# )\n",
    "\n",
    "env_name = \"CartPole-v1\" # @param {type:\"string\"}\n",
    "num_iterations = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 100  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (64,64)\n",
    "\n",
    "batch_size = 128  # @param {type:\"integer\"}\n",
    "learning_rate = 3e-4  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "\n",
    "actor_fc_layers = (8,16,32,64,128,64,32,16,8,4,2)\n",
    "value_fc_layers = (8,16,32,64,128,64,128,64,32,16,8,4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_tensor_spec = tensor_spec.from_spec(train_env.observation_spec())\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "time_step_tensor_spec = tensor_spec.from_spec(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "Action: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TimeStep: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('Observation: {0}'.format(observation_tensor_spec))\n",
    "print('Action: {0}'.format(action_tensor_spec))\n",
    "print('TimeStep: {0}'.format(time_step_tensor_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netowork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_fc_layers = (64,64)\n",
    "value_fc_layers = (64, 64)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_tensor_spec,\n",
    "        action_tensor_spec,\n",
    "        fc_layer_params=actor_fc_layers,\n",
    "        activation_fn=tf.keras.activations.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.Orthogonal(seed=1),\n",
    "        seed_stream_class=tfp.util.SeedStream\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print(observation_tensor_spec)\n",
    "print(action_tensor_spec)\n",
    "print(time_step_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 16:22:15.754206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "epsilon = 0.2\n",
    "discount = 0.99\n",
    "gae_lambda = 0.95\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent_ppo = ppo_clip_agent.PPOClipAgent(\n",
    "    time_step_spec=time_step_tensor_spec,\n",
    "    action_spec=action_tensor_spec,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=optimizer,\n",
    "    use_gae=True,\n",
    "    lambda_value=gae_lambda,\n",
    "    importance_ratio_clipping=epsilon,\n",
    "    # log_prob_clipping=epsilon,\n",
    "    # gradient_clipping=epsilon,\n",
    "    # value_clipping=epsilon,\n",
    "    train_step_counter=train_step_counter,\n",
    ")\n",
    "agent_ppo.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(\n{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04596459, -0.04368313,  0.04124061,  0.04081279]],\n      dtype=float32)>,\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    info_spec=time_step_tensor_spec)\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "  for _ in range(1):\n",
    "    avg= compute_avg_return(eval_env, policy, num_eval_episodes)\n",
    "    print(avg)\n",
    "\n",
    "# evaluate_policy(random_policy)\n",
    "\n",
    "# evaluate_policy(agent_ppo.policy)\n",
    "\n",
    "evaluate_policy(agent_ppo.collect_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent_ppo.collect_policy.trajectory_spec,\n",
    "    batch_size=1,\n",
    "    max_length=10000)\n",
    "    # max_length=10000)\n",
    "\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "    # print(next_time_step.reward)\n",
    "\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(batch_size)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# print(f'TimeStep: {train_env.time_step_spec()}')\n",
    "# print(f'Action: {train_env.action_spec()}')\n",
    "# print(f'Agent: {agent_ppo.collect_policy.time_step_spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 50: loss = 88.1902847290039  AVG RETURN: 500.000000\n",
      "step = 100: loss = 78.51990509033203  AVG RETURN: 495.200012\n",
      "step = 150: loss = 82.6442642211914  AVG RETURN: 500.000000\n",
      "step = 200: loss = 89.36605072021484  AVG RETURN: 500.000000\n",
      "step = 250: loss = 85.83831024169922  AVG RETURN: 495.700012\n",
      "step = 300: loss = 87.60682678222656  AVG RETURN: 500.000000\n",
      "step = 350: loss = 88.50658416748047  AVG RETURN: 499.700012\n",
      "step = 400: loss = 74.34912872314453  AVG RETURN: 500.000000\n",
      "step = 450: loss = 94.97447204589844  AVG RETURN: 500.000000\n",
      "step = 500: loss = 85.76362609863281  AVG RETURN: 500.000000\n",
      "step = 550: loss = 76.45671844482422  AVG RETURN: 497.299988\n",
      "step = 600: loss = 65.9117431640625  AVG RETURN: 483.299988\n",
      "step = 650: loss = 69.25669860839844  AVG RETURN: 495.600006\n",
      "step = 700: loss = 88.40122985839844  AVG RETURN: 493.799988\n",
      "step = 750: loss = 86.21687316894531  AVG RETURN: 496.299988\n",
      "step = 800: loss = 69.6532974243164  AVG RETURN: 500.000000\n",
      "step = 850: loss = 76.07813262939453  AVG RETURN: 500.000000\n",
      "step = 900: loss = 72.40042877197266  AVG RETURN: 493.200012\n",
      "step = 950: loss = 72.10818481445312  AVG RETURN: 500.000000\n",
      "step = 1000: loss = 79.54463195800781\n",
      "step = 1000: loss = 79.54463195800781  AVG RETURN: 461.799988\n",
      "step = 1050: loss = 101.24313354492188  AVG RETURN: 477.799988\n",
      "step = 1100: loss = 94.29180908203125  AVG RETURN: 491.000000\n",
      "step = 1150: loss = 84.64521789550781  AVG RETURN: 472.200012\n",
      "step = 1200: loss = 111.78675842285156  AVG RETURN: 481.799988\n",
      "step = 1250: loss = 65.86100006103516  AVG RETURN: 487.700012\n",
      "step = 1300: loss = 69.1338119506836  AVG RETURN: 482.799988\n",
      "step = 1350: loss = 79.90618133544922  AVG RETURN: 500.000000\n",
      "step = 1400: loss = 69.4090347290039  AVG RETURN: 500.000000\n",
      "step = 1450: loss = 66.66222381591797  AVG RETURN: 500.000000\n",
      "step = 1500: loss = 67.79522705078125  AVG RETURN: 479.100006\n",
      "step = 1550: loss = 64.82794952392578  AVG RETURN: 484.100006\n",
      "step = 1600: loss = 55.104488372802734  AVG RETURN: 500.000000\n",
      "step = 1650: loss = 66.66686248779297  AVG RETURN: 500.000000\n",
      "step = 1700: loss = 56.371395111083984  AVG RETURN: 500.000000\n",
      "step = 1750: loss = 69.6163330078125  AVG RETURN: 500.000000\n",
      "step = 1800: loss = 90.88282012939453  AVG RETURN: 494.000000\n",
      "step = 1850: loss = 90.72355651855469  AVG RETURN: 500.000000\n",
      "step = 1900: loss = 82.44918060302734  AVG RETURN: 500.000000\n",
      "step = 1950: loss = 76.1510009765625  AVG RETURN: 500.000000\n",
      "step = 2000: loss = 80.14348602294922\n",
      "step = 2000: loss = 80.14348602294922  AVG RETURN: 500.000000\n",
      "step = 2050: loss = 72.68995666503906  AVG RETURN: 500.000000\n",
      "step = 2100: loss = 75.60616302490234  AVG RETURN: 500.000000\n",
      "step = 2150: loss = 88.3975601196289  AVG RETURN: 500.000000\n",
      "step = 2200: loss = 83.38994598388672  AVG RETURN: 500.000000\n",
      "step = 2250: loss = 103.29826354980469  AVG RETURN: 498.399994\n",
      "step = 2300: loss = 106.82513427734375  AVG RETURN: 500.000000\n",
      "step = 2350: loss = 92.13841247558594  AVG RETURN: 500.000000\n",
      "step = 2400: loss = 88.32957458496094  AVG RETURN: 500.000000\n",
      "step = 2450: loss = 107.69457244873047  AVG RETURN: 500.000000\n",
      "step = 2500: loss = 60.62691879272461  AVG RETURN: 500.000000\n",
      "step = 2550: loss = 93.64457702636719  AVG RETURN: 500.000000\n",
      "step = 2600: loss = 66.49188232421875  AVG RETURN: 496.000000\n",
      "step = 2650: loss = 74.91083526611328  AVG RETURN: 500.000000\n",
      "step = 2700: loss = 101.73428344726562  AVG RETURN: 500.000000\n",
      "step = 2750: loss = 82.14126586914062  AVG RETURN: 500.000000\n",
      "step = 2800: loss = 68.31318664550781  AVG RETURN: 500.000000\n",
      "step = 2850: loss = 66.92481994628906  AVG RETURN: 499.399994\n",
      "step = 2900: loss = 83.1214599609375  AVG RETURN: 500.000000\n",
      "step = 2950: loss = 134.41554260253906  AVG RETURN: 493.399994\n",
      "step = 3000: loss = 94.07347869873047\n",
      "step = 3000: loss = 94.07347869873047  AVG RETURN: 500.000000\n",
      "step = 3050: loss = 76.2939682006836  AVG RETURN: 500.000000\n",
      "step = 3100: loss = 80.54985809326172  AVG RETURN: 497.700012\n",
      "step = 3150: loss = 70.6709213256836  AVG RETURN: 500.000000\n",
      "step = 3200: loss = 86.3920669555664  AVG RETURN: 496.100006\n",
      "step = 3250: loss = 80.43035888671875  AVG RETURN: 500.000000\n",
      "step = 3300: loss = 74.79487609863281  AVG RETURN: 500.000000\n",
      "step = 3350: loss = 78.55200958251953  AVG RETURN: 500.000000\n",
      "step = 3400: loss = 113.48809814453125  AVG RETURN: 500.000000\n",
      "step = 3450: loss = 72.9961929321289  AVG RETURN: 500.000000\n",
      "step = 3500: loss = 85.00494384765625  AVG RETURN: 500.000000\n",
      "step = 3550: loss = 103.75823974609375  AVG RETURN: 500.000000\n",
      "step = 3600: loss = 88.643310546875  AVG RETURN: 500.000000\n",
      "step = 3650: loss = 77.06682586669922  AVG RETURN: 500.000000\n",
      "step = 3700: loss = 82.70729064941406  AVG RETURN: 500.000000\n",
      "step = 3750: loss = 79.27435302734375  AVG RETURN: 500.000000\n",
      "step = 3800: loss = 59.021629333496094  AVG RETURN: 500.000000\n",
      "step = 3850: loss = 65.78337097167969  AVG RETURN: 500.000000\n",
      "step = 3900: loss = 102.0899658203125  AVG RETURN: 500.000000\n",
      "step = 3950: loss = 81.33917999267578  AVG RETURN: 500.000000\n",
      "step = 4000: loss = 102.15351104736328\n",
      "step = 4000: loss = 102.15351104736328  AVG RETURN: 500.000000\n",
      "step = 4050: loss = 109.2896957397461  AVG RETURN: 500.000000\n",
      "step = 4100: loss = 86.69391632080078  AVG RETURN: 497.200012\n",
      "step = 4150: loss = 109.01049041748047  AVG RETURN: 500.000000\n",
      "step = 4200: loss = 62.272159576416016  AVG RETURN: 500.000000\n",
      "step = 4250: loss = 87.65591430664062  AVG RETURN: 500.000000\n",
      "step = 4300: loss = 78.37744903564453  AVG RETURN: 496.000000\n",
      "step = 4350: loss = 63.541446685791016  AVG RETURN: 500.000000\n",
      "step = 4400: loss = 86.16000366210938  AVG RETURN: 500.000000\n",
      "step = 4450: loss = 83.93441009521484  AVG RETURN: 500.000000\n",
      "step = 4500: loss = 105.85755920410156  AVG RETURN: 500.000000\n",
      "step = 4550: loss = 106.08683776855469  AVG RETURN: 500.000000\n",
      "step = 4600: loss = 81.6460189819336  AVG RETURN: 500.000000\n",
      "step = 4650: loss = 82.2919692993164  AVG RETURN: 497.600006\n",
      "step = 4700: loss = 97.4271011352539  AVG RETURN: 500.000000\n",
      "step = 4750: loss = 85.14116668701172  AVG RETURN: 496.000000\n",
      "step = 4800: loss = 111.2298583984375  AVG RETURN: 500.000000\n",
      "step = 4850: loss = 77.31859588623047  AVG RETURN: 493.799988\n",
      "step = 4900: loss = 116.40557861328125  AVG RETURN: 500.000000\n",
      "step = 4950: loss = 92.95520782470703  AVG RETURN: 493.299988\n",
      "step = 5000: loss = 88.308349609375\n",
      "step = 5000: loss = 88.308349609375  AVG RETURN: 484.000000\n",
      "step = 5050: loss = 105.52543640136719  AVG RETURN: 500.000000\n",
      "step = 5100: loss = 98.85293579101562  AVG RETURN: 493.899994\n",
      "step = 5150: loss = 99.01261901855469  AVG RETURN: 492.799988\n",
      "step = 5200: loss = 88.60269927978516  AVG RETURN: 500.000000\n",
      "step = 5250: loss = 67.91963958740234  AVG RETURN: 491.399994\n",
      "step = 5300: loss = 95.58403778076172  AVG RETURN: 485.299988\n",
      "step = 5350: loss = 139.6248321533203  AVG RETURN: 500.000000\n",
      "step = 5400: loss = 101.99644470214844  AVG RETURN: 494.700012\n",
      "step = 5450: loss = 98.6870346069336  AVG RETURN: 500.000000\n",
      "step = 5500: loss = 112.39488983154297  AVG RETURN: 500.000000\n",
      "step = 5550: loss = 92.76020812988281  AVG RETURN: 500.000000\n",
      "step = 5600: loss = 116.15191650390625  AVG RETURN: 500.000000\n",
      "step = 5650: loss = 59.66427993774414  AVG RETURN: 487.700012\n",
      "step = 5700: loss = 112.07231140136719  AVG RETURN: 500.000000\n",
      "step = 5750: loss = 100.26599884033203  AVG RETURN: 472.600006\n",
      "step = 5800: loss = 100.9528579711914  AVG RETURN: 500.000000\n",
      "step = 5850: loss = 103.12596893310547  AVG RETURN: 500.000000\n",
      "step = 5900: loss = 114.60910034179688  AVG RETURN: 500.000000\n",
      "step = 5950: loss = 69.02820587158203  AVG RETURN: 500.000000\n",
      "step = 6000: loss = 130.67807006835938\n",
      "step = 6000: loss = 130.67807006835938  AVG RETURN: 500.000000\n",
      "step = 6050: loss = 127.62387084960938  AVG RETURN: 496.000000\n",
      "step = 6100: loss = 167.93385314941406  AVG RETURN: 493.500000\n",
      "step = 6150: loss = 99.37944030761719  AVG RETURN: 494.299988\n",
      "step = 6200: loss = 97.39585876464844  AVG RETURN: 500.000000\n",
      "step = 6250: loss = 98.6515121459961  AVG RETURN: 500.000000\n",
      "step = 6300: loss = 115.16539764404297  AVG RETURN: 500.000000\n",
      "step = 6350: loss = 93.08792877197266  AVG RETURN: 500.000000\n",
      "step = 6400: loss = 108.52092742919922  AVG RETURN: 500.000000\n",
      "step = 6450: loss = 100.99356079101562  AVG RETURN: 500.000000\n",
      "step = 6500: loss = 96.52042388916016  AVG RETURN: 500.000000\n",
      "step = 6550: loss = 92.45525360107422  AVG RETURN: 500.000000\n",
      "step = 6600: loss = 77.00318145751953  AVG RETURN: 500.000000\n",
      "step = 6650: loss = 81.17628479003906  AVG RETURN: 500.000000\n",
      "step = 6700: loss = 102.91881561279297  AVG RETURN: 499.399994\n",
      "step = 6750: loss = 105.59895324707031  AVG RETURN: 495.399994\n",
      "step = 6800: loss = 104.791015625  AVG RETURN: 493.299988\n",
      "step = 6850: loss = 91.15464782714844  AVG RETURN: 500.000000\n",
      "step = 6900: loss = 96.1738052368164  AVG RETURN: 496.000000\n",
      "step = 6950: loss = 113.70366668701172  AVG RETURN: 500.000000\n",
      "step = 7000: loss = 97.23998260498047\n",
      "step = 7000: loss = 97.23998260498047  AVG RETURN: 500.000000\n",
      "step = 7050: loss = 83.16316223144531  AVG RETURN: 500.000000\n",
      "step = 7100: loss = 95.4896469116211  AVG RETURN: 499.100006\n",
      "step = 7150: loss = 100.54306030273438  AVG RETURN: 500.000000\n",
      "step = 7200: loss = 84.06108093261719  AVG RETURN: 500.000000\n",
      "step = 7250: loss = 92.8074722290039  AVG RETURN: 500.000000\n",
      "step = 7300: loss = 96.40605926513672  AVG RETURN: 500.000000\n",
      "step = 7350: loss = 72.27193450927734  AVG RETURN: 500.000000\n",
      "step = 7400: loss = 110.62883758544922  AVG RETURN: 500.000000\n",
      "step = 7450: loss = 124.77043914794922  AVG RETURN: 500.000000\n",
      "step = 7500: loss = 106.85321044921875  AVG RETURN: 500.000000\n",
      "step = 7550: loss = 86.24181365966797  AVG RETURN: 500.000000\n",
      "step = 7600: loss = 130.97454833984375  AVG RETURN: 500.000000\n",
      "step = 7650: loss = 121.58529663085938  AVG RETURN: 500.000000\n",
      "step = 7700: loss = 179.40272521972656  AVG RETURN: 500.000000\n",
      "step = 7750: loss = 122.70429992675781  AVG RETURN: 500.000000\n",
      "step = 7800: loss = 112.03543090820312  AVG RETURN: 500.000000\n",
      "step = 7850: loss = 141.33059692382812  AVG RETURN: 500.000000\n",
      "step = 7900: loss = 100.9081039428711  AVG RETURN: 500.000000\n",
      "step = 7950: loss = 101.01189422607422  AVG RETURN: 500.000000\n",
      "step = 8000: loss = 92.874755859375\n",
      "step = 8000: loss = 92.874755859375  AVG RETURN: 500.000000\n",
      "step = 8050: loss = 70.23056030273438  AVG RETURN: 500.000000\n",
      "step = 8100: loss = 92.66490173339844  AVG RETURN: 500.000000\n",
      "step = 8150: loss = 133.41244506835938  AVG RETURN: 500.000000\n",
      "step = 8200: loss = 93.81170654296875  AVG RETURN: 500.000000\n",
      "step = 8250: loss = 122.58544158935547  AVG RETURN: 500.000000\n",
      "step = 8300: loss = 88.01937866210938  AVG RETURN: 500.000000\n",
      "step = 8350: loss = 99.64214324951172  AVG RETURN: 500.000000\n",
      "step = 8400: loss = 88.414794921875  AVG RETURN: 500.000000\n",
      "step = 8450: loss = 71.36795043945312  AVG RETURN: 500.000000\n",
      "step = 8500: loss = 105.66488647460938  AVG RETURN: 500.000000\n",
      "step = 8550: loss = 116.61189270019531  AVG RETURN: 500.000000\n",
      "step = 8600: loss = 114.3139419555664  AVG RETURN: 500.000000\n",
      "step = 8650: loss = 117.66683959960938  AVG RETURN: 500.000000\n",
      "step = 8700: loss = 103.24715423583984  AVG RETURN: 500.000000\n",
      "step = 8750: loss = 128.92349243164062  AVG RETURN: 500.000000\n",
      "step = 8800: loss = 86.6446762084961  AVG RETURN: 500.000000\n",
      "step = 8850: loss = 93.42452239990234  AVG RETURN: 500.000000\n",
      "step = 8900: loss = 105.55375671386719  AVG RETURN: 500.000000\n",
      "step = 8950: loss = 124.73455810546875  AVG RETURN: 500.000000\n",
      "step = 9000: loss = 99.02253723144531\n",
      "step = 9000: loss = 99.02253723144531  AVG RETURN: 500.000000\n",
      "step = 9050: loss = 109.04520416259766  AVG RETURN: 500.000000\n",
      "step = 9100: loss = 115.8879165649414  AVG RETURN: 500.000000\n",
      "step = 9150: loss = 81.20659637451172  AVG RETURN: 500.000000\n",
      "step = 9200: loss = 127.92192840576172  AVG RETURN: 500.000000\n",
      "step = 9250: loss = 90.32186126708984  AVG RETURN: 500.000000\n",
      "step = 9300: loss = 138.2291259765625  AVG RETURN: 500.000000\n",
      "step = 9350: loss = 137.72879028320312  AVG RETURN: 500.000000\n",
      "step = 9400: loss = 108.68592834472656  AVG RETURN: 496.500000\n",
      "step = 9450: loss = 111.84831237792969  AVG RETURN: 500.000000\n",
      "step = 9500: loss = 103.97386169433594  AVG RETURN: 500.000000\n",
      "step = 9550: loss = 97.72154235839844  AVG RETURN: 500.000000\n",
      "step = 9600: loss = 81.958740234375  AVG RETURN: 500.000000\n",
      "step = 9650: loss = 103.92861938476562  AVG RETURN: 500.000000\n",
      "step = 9700: loss = 104.67528533935547  AVG RETURN: 500.000000\n",
      "step = 9750: loss = 108.60936737060547  AVG RETURN: 500.000000\n",
      "step = 9800: loss = 93.77088928222656  AVG RETURN: 500.000000\n",
      "step = 9850: loss = 119.0204849243164  AVG RETURN: 500.000000\n",
      "step = 9900: loss = 77.04586791992188  AVG RETURN: 500.000000\n",
      "step = 9950: loss = 128.88511657714844  AVG RETURN: 500.000000\n",
      "step = 10000: loss = 124.68728637695312\n",
      "step = 10000: loss = 124.68728637695312  AVG RETURN: 500.000000\n",
      "step = 10050: loss = 142.14669799804688  AVG RETURN: 500.000000\n",
      "step = 10100: loss = 132.68829345703125  AVG RETURN: 500.000000\n",
      "step = 10150: loss = 124.59019470214844  AVG RETURN: 500.000000\n",
      "step = 10200: loss = 86.24785614013672  AVG RETURN: 500.000000\n",
      "step = 10250: loss = 100.222900390625  AVG RETURN: 500.000000\n",
      "step = 10300: loss = 92.36540222167969  AVG RETURN: 500.000000\n",
      "step = 10350: loss = 104.27694702148438  AVG RETURN: 500.000000\n",
      "step = 10400: loss = 89.03308868408203  AVG RETURN: 500.000000\n",
      "step = 10450: loss = 100.36726379394531  AVG RETURN: 500.000000\n",
      "step = 10500: loss = 94.26202392578125  AVG RETURN: 500.000000\n",
      "step = 10550: loss = 108.86101531982422  AVG RETURN: 500.000000\n",
      "step = 10600: loss = 114.65704345703125  AVG RETURN: 500.000000\n",
      "step = 10650: loss = 99.16978454589844  AVG RETURN: 500.000000\n",
      "step = 10700: loss = 132.57427978515625  AVG RETURN: 500.000000\n",
      "step = 10750: loss = 141.84085083007812  AVG RETURN: 500.000000\n",
      "step = 10800: loss = 84.93251037597656  AVG RETURN: 500.000000\n",
      "step = 10850: loss = 120.2293472290039  AVG RETURN: 500.000000\n",
      "step = 10900: loss = 129.99732971191406  AVG RETURN: 500.000000\n",
      "step = 10950: loss = 127.64295959472656  AVG RETURN: 500.000000\n",
      "step = 11000: loss = 145.32742309570312\n",
      "step = 11000: loss = 145.32742309570312  AVG RETURN: 500.000000\n",
      "step = 11050: loss = 127.95282745361328  AVG RETURN: 500.000000\n",
      "step = 11100: loss = 88.28514862060547  AVG RETURN: 500.000000\n",
      "step = 11150: loss = 123.3801040649414  AVG RETURN: 500.000000\n",
      "step = 11200: loss = 96.7001724243164  AVG RETURN: 500.000000\n",
      "step = 11250: loss = 100.52591705322266  AVG RETURN: 500.000000\n",
      "step = 11300: loss = 103.36798095703125  AVG RETURN: 500.000000\n",
      "step = 11350: loss = 103.67083740234375  AVG RETURN: 500.000000\n",
      "step = 11400: loss = 101.26790618896484  AVG RETURN: 500.000000\n",
      "step = 11450: loss = 99.30396270751953  AVG RETURN: 500.000000\n",
      "step = 11500: loss = 125.01337432861328  AVG RETURN: 500.000000\n",
      "step = 11550: loss = 119.70378112792969  AVG RETURN: 500.000000\n",
      "step = 11600: loss = 94.2769775390625  AVG RETURN: 500.000000\n",
      "step = 11650: loss = 118.0967025756836  AVG RETURN: 500.000000\n",
      "step = 11700: loss = 115.05747985839844  AVG RETURN: 500.000000\n",
      "step = 11750: loss = 104.05486297607422  AVG RETURN: 500.000000\n",
      "step = 11800: loss = 153.2801971435547  AVG RETURN: 500.000000\n",
      "step = 11850: loss = 94.49246215820312  AVG RETURN: 500.000000\n",
      "step = 11900: loss = 105.04554748535156  AVG RETURN: 500.000000\n",
      "step = 11950: loss = 78.9489517211914  AVG RETURN: 500.000000\n",
      "step = 12000: loss = 93.93216705322266\n",
      "step = 12000: loss = 93.93216705322266  AVG RETURN: 500.000000\n",
      "step = 12050: loss = 111.68404388427734  AVG RETURN: 500.000000\n",
      "step = 12100: loss = 109.8892822265625  AVG RETURN: 500.000000\n",
      "step = 12150: loss = 83.2365951538086  AVG RETURN: 500.000000\n",
      "step = 12200: loss = 132.21456909179688  AVG RETURN: 500.000000\n",
      "step = 12250: loss = 204.60292053222656  AVG RETURN: 500.000000\n",
      "step = 12300: loss = 89.685546875  AVG RETURN: 500.000000\n",
      "step = 12350: loss = 109.6262435913086  AVG RETURN: 500.000000\n",
      "step = 12400: loss = 113.30614471435547  AVG RETURN: 500.000000\n",
      "step = 12450: loss = 133.50950622558594  AVG RETURN: 500.000000\n",
      "step = 12500: loss = 93.53060150146484  AVG RETURN: 500.000000\n",
      "step = 12550: loss = 132.8522491455078  AVG RETURN: 500.000000\n",
      "step = 12600: loss = 143.49237060546875  AVG RETURN: 500.000000\n",
      "step = 12650: loss = 135.90310668945312  AVG RETURN: 500.000000\n",
      "step = 12700: loss = 105.49337768554688  AVG RETURN: 500.000000\n",
      "step = 12750: loss = 108.6976547241211  AVG RETURN: 500.000000\n",
      "step = 12800: loss = 144.3744659423828  AVG RETURN: 500.000000\n",
      "step = 12850: loss = 118.89955139160156  AVG RETURN: 500.000000\n",
      "step = 12900: loss = 101.89009094238281  AVG RETURN: 500.000000\n",
      "step = 12950: loss = 80.76696014404297  AVG RETURN: 500.000000\n",
      "step = 13000: loss = 106.7801742553711\n",
      "step = 13000: loss = 106.7801742553711  AVG RETURN: 500.000000\n",
      "step = 13050: loss = 105.01338958740234  AVG RETURN: 500.000000\n",
      "step = 13100: loss = 113.26763916015625  AVG RETURN: 500.000000\n",
      "step = 13150: loss = 116.82975769042969  AVG RETURN: 500.000000\n",
      "step = 13200: loss = 120.9552993774414  AVG RETURN: 500.000000\n",
      "step = 13250: loss = 92.71470642089844  AVG RETURN: 499.100006\n",
      "step = 13300: loss = 107.65225982666016  AVG RETURN: 500.000000\n",
      "step = 13350: loss = 121.34196472167969  AVG RETURN: 500.000000\n",
      "step = 13400: loss = 106.05054473876953  AVG RETURN: 500.000000\n",
      "step = 13450: loss = 105.48945617675781  AVG RETURN: 500.000000\n",
      "step = 13500: loss = 153.802490234375  AVG RETURN: 500.000000\n",
      "step = 13550: loss = 110.16378784179688  AVG RETURN: 500.000000\n",
      "step = 13600: loss = 107.5430679321289  AVG RETURN: 500.000000\n",
      "step = 13650: loss = 140.5260772705078  AVG RETURN: 500.000000\n",
      "step = 13700: loss = 74.53268432617188  AVG RETURN: 500.000000\n",
      "step = 13750: loss = 122.715576171875  AVG RETURN: 500.000000\n",
      "step = 13800: loss = 128.80902099609375  AVG RETURN: 500.000000\n",
      "step = 13850: loss = 155.00209045410156  AVG RETURN: 500.000000\n",
      "step = 13900: loss = 124.12199401855469  AVG RETURN: 500.000000\n",
      "step = 13950: loss = 86.95037841796875  AVG RETURN: 500.000000\n",
      "step = 14000: loss = 111.19606018066406\n",
      "step = 14000: loss = 111.19606018066406  AVG RETURN: 500.000000\n",
      "step = 14050: loss = 120.41764068603516  AVG RETURN: 500.000000\n",
      "step = 14100: loss = 143.30198669433594  AVG RETURN: 500.000000\n",
      "step = 14150: loss = 154.52146911621094  AVG RETURN: 500.000000\n",
      "step = 14200: loss = 107.6854248046875  AVG RETURN: 500.000000\n",
      "step = 14250: loss = 130.3079833984375  AVG RETURN: 500.000000\n",
      "step = 14300: loss = 130.224365234375  AVG RETURN: 500.000000\n",
      "step = 14350: loss = 78.92850494384766  AVG RETURN: 500.000000\n",
      "step = 14400: loss = 96.69369506835938  AVG RETURN: 500.000000\n",
      "step = 14450: loss = 231.21478271484375  AVG RETURN: 500.000000\n",
      "step = 14500: loss = 109.68789672851562  AVG RETURN: 500.000000\n",
      "step = 14550: loss = 149.02317810058594  AVG RETURN: 500.000000\n",
      "step = 14600: loss = 112.12417602539062  AVG RETURN: 500.000000\n",
      "step = 14650: loss = 120.47000885009766  AVG RETURN: 500.000000\n",
      "step = 14700: loss = 98.19424438476562  AVG RETURN: 500.000000\n",
      "step = 14750: loss = 142.77212524414062  AVG RETURN: 500.000000\n",
      "step = 14800: loss = 110.60491180419922  AVG RETURN: 500.000000\n",
      "step = 14850: loss = 128.99742126464844  AVG RETURN: 500.000000\n",
      "step = 14900: loss = 116.0460433959961  AVG RETURN: 500.000000\n",
      "step = 14950: loss = 92.54103088378906  AVG RETURN: 500.000000\n",
      "step = 15000: loss = 161.8444061279297\n",
      "step = 15000: loss = 161.8444061279297  AVG RETURN: 500.000000\n",
      "step = 15050: loss = 146.1493377685547  AVG RETURN: 500.000000\n",
      "step = 15100: loss = 199.23130798339844  AVG RETURN: 500.000000\n",
      "step = 15150: loss = 158.2801971435547  AVG RETURN: 497.799988\n",
      "step = 15200: loss = 145.27972412109375  AVG RETURN: 500.000000\n",
      "step = 15250: loss = 128.93753051757812  AVG RETURN: 500.000000\n",
      "step = 15300: loss = 99.89500427246094  AVG RETURN: 500.000000\n",
      "step = 15350: loss = 115.65521240234375  AVG RETURN: 500.000000\n",
      "step = 15400: loss = 158.97300720214844  AVG RETURN: 500.000000\n",
      "step = 15450: loss = 108.68791198730469  AVG RETURN: 500.000000\n",
      "step = 15500: loss = 181.41189575195312  AVG RETURN: 500.000000\n",
      "step = 15550: loss = 157.970947265625  AVG RETURN: 500.000000\n",
      "step = 15600: loss = 172.2801971435547  AVG RETURN: 500.000000\n",
      "step = 15650: loss = 151.30364990234375  AVG RETURN: 500.000000\n",
      "step = 15700: loss = 171.04168701171875  AVG RETURN: 500.000000\n",
      "step = 15750: loss = 150.3503875732422  AVG RETURN: 500.000000\n",
      "step = 15800: loss = 174.1787872314453  AVG RETURN: 500.000000\n",
      "step = 15850: loss = 143.3324432373047  AVG RETURN: 500.000000\n",
      "step = 15900: loss = 154.24192810058594  AVG RETURN: 500.000000\n",
      "step = 15950: loss = 149.36062622070312  AVG RETURN: 500.000000\n",
      "step = 16000: loss = 174.46539306640625\n",
      "step = 16000: loss = 174.46539306640625  AVG RETURN: 500.000000\n",
      "step = 16050: loss = 176.92657470703125  AVG RETURN: 500.000000\n",
      "step = 16100: loss = 117.97614288330078  AVG RETURN: 500.000000\n",
      "step = 16150: loss = 114.20878601074219  AVG RETURN: 500.000000\n",
      "step = 16200: loss = 93.82723999023438  AVG RETURN: 500.000000\n",
      "step = 16250: loss = 268.1213684082031  AVG RETURN: 500.000000\n",
      "step = 16300: loss = 152.9932403564453  AVG RETURN: 500.000000\n",
      "step = 16350: loss = 332.1656188964844  AVG RETURN: 500.000000\n",
      "step = 16400: loss = 121.42815399169922  AVG RETURN: 500.000000\n",
      "step = 16450: loss = 147.8639678955078  AVG RETURN: 500.000000\n",
      "step = 16500: loss = 134.71456909179688  AVG RETURN: 500.000000\n",
      "step = 16550: loss = 142.78765869140625  AVG RETURN: 500.000000\n",
      "step = 16600: loss = 120.81880187988281  AVG RETURN: 500.000000\n",
      "step = 16650: loss = 346.63739013671875  AVG RETURN: 500.000000\n",
      "step = 16700: loss = 165.27841186523438  AVG RETURN: 499.399994\n",
      "step = 16750: loss = 200.77413940429688  AVG RETURN: 496.299988\n",
      "step = 16800: loss = 160.30288696289062  AVG RETURN: 500.000000\n",
      "step = 16850: loss = 153.50411987304688  AVG RETURN: 500.000000\n",
      "step = 16900: loss = 160.5187225341797  AVG RETURN: 500.000000\n",
      "step = 16950: loss = 180.740478515625  AVG RETURN: 500.000000\n",
      "step = 17000: loss = 154.06175231933594\n",
      "step = 17000: loss = 154.06175231933594  AVG RETURN: 498.399994\n",
      "step = 17050: loss = 144.49664306640625  AVG RETURN: 500.000000\n",
      "step = 17100: loss = 136.31997680664062  AVG RETURN: 500.000000\n",
      "step = 17150: loss = 214.59165954589844  AVG RETURN: 500.000000\n",
      "step = 17200: loss = 124.53665924072266  AVG RETURN: 485.799988\n",
      "step = 17250: loss = 121.49798583984375  AVG RETURN: 500.000000\n",
      "step = 17300: loss = 144.76748657226562  AVG RETURN: 490.000000\n",
      "step = 17350: loss = 153.28610229492188  AVG RETURN: 500.000000\n",
      "step = 17400: loss = 181.94921875  AVG RETURN: 500.000000\n",
      "step = 17450: loss = 175.0556182861328  AVG RETURN: 500.000000\n",
      "step = 17500: loss = 201.4571533203125  AVG RETURN: 491.200012\n",
      "step = 17550: loss = 178.84866333007812  AVG RETURN: 500.000000\n",
      "step = 17600: loss = 136.50572204589844  AVG RETURN: 500.000000\n",
      "step = 17650: loss = 127.51719665527344  AVG RETURN: 500.000000\n",
      "step = 17700: loss = 152.59243774414062  AVG RETURN: 500.000000\n",
      "step = 17750: loss = 125.42830657958984  AVG RETURN: 500.000000\n",
      "step = 17800: loss = 124.34337615966797  AVG RETURN: 500.000000\n",
      "step = 17850: loss = 107.10737609863281  AVG RETURN: 500.000000\n",
      "step = 17900: loss = 131.37489318847656  AVG RETURN: 500.000000\n",
      "step = 17950: loss = 129.94602966308594  AVG RETURN: 496.100006\n",
      "step = 18000: loss = 213.98715209960938\n",
      "step = 18000: loss = 213.98715209960938  AVG RETURN: 498.899994\n",
      "step = 18050: loss = 157.47222900390625  AVG RETURN: 500.000000\n",
      "step = 18100: loss = 171.08460998535156  AVG RETURN: 500.000000\n",
      "step = 18150: loss = 162.30453491210938  AVG RETURN: 500.000000\n",
      "step = 18200: loss = 193.81460571289062  AVG RETURN: 500.000000\n",
      "step = 18250: loss = 223.5192108154297  AVG RETURN: 495.500000\n",
      "step = 18300: loss = 162.22659301757812  AVG RETURN: 497.000000\n",
      "step = 18350: loss = 152.39492797851562  AVG RETURN: 500.000000\n",
      "step = 18400: loss = 111.70581817626953  AVG RETURN: 500.000000\n",
      "step = 18450: loss = 119.1447525024414  AVG RETURN: 500.000000\n",
      "step = 18500: loss = 112.94561004638672  AVG RETURN: 500.000000\n",
      "step = 18550: loss = 162.9467315673828  AVG RETURN: 500.000000\n",
      "step = 18600: loss = 119.98526000976562  AVG RETURN: 500.000000\n",
      "step = 18650: loss = 165.75997924804688  AVG RETURN: 500.000000\n",
      "step = 18700: loss = 120.63874053955078  AVG RETURN: 500.000000\n",
      "step = 18750: loss = 133.91567993164062  AVG RETURN: 480.500000\n",
      "step = 18800: loss = 232.41368103027344  AVG RETURN: 496.700012\n",
      "step = 18850: loss = 113.71260070800781  AVG RETURN: 473.399994\n",
      "step = 18900: loss = 138.03443908691406  AVG RETURN: 500.000000\n",
      "step = 18950: loss = 123.65278625488281  AVG RETURN: 498.500000\n",
      "step = 19000: loss = 241.07858276367188\n",
      "step = 19000: loss = 241.07858276367188  AVG RETURN: 490.200012\n",
      "step = 19050: loss = 177.84368896484375  AVG RETURN: 500.000000\n",
      "step = 19100: loss = 186.49037170410156  AVG RETURN: 500.000000\n",
      "step = 19150: loss = 178.99371337890625  AVG RETURN: 500.000000\n",
      "step = 19200: loss = 119.46406555175781  AVG RETURN: 500.000000\n",
      "step = 19250: loss = 115.64812469482422  AVG RETURN: 500.000000\n",
      "step = 19300: loss = 147.66073608398438  AVG RETURN: 500.000000\n",
      "step = 19350: loss = 136.0829315185547  AVG RETURN: 500.000000\n",
      "step = 19400: loss = 190.97946166992188  AVG RETURN: 500.000000\n",
      "step = 19450: loss = 162.85910034179688  AVG RETURN: 500.000000\n",
      "step = 19500: loss = 165.9165496826172  AVG RETURN: 500.000000\n",
      "step = 19550: loss = 127.95198822021484  AVG RETURN: 500.000000\n",
      "step = 19600: loss = 134.74884033203125  AVG RETURN: 500.000000\n",
      "step = 19650: loss = 179.57496643066406  AVG RETURN: 500.000000\n",
      "step = 19700: loss = 148.3713836669922  AVG RETURN: 500.000000\n",
      "step = 19750: loss = 100.9930648803711  AVG RETURN: 500.000000\n",
      "step = 19800: loss = 138.5400390625  AVG RETURN: 500.000000\n",
      "step = 19850: loss = 194.98670959472656  AVG RETURN: 500.000000\n",
      "step = 19900: loss = 153.21717834472656  AVG RETURN: 500.000000\n",
      "step = 19950: loss = 192.64529418945312  AVG RETURN: 500.000000\n",
      "step = 20000: loss = 200.77127075195312\n",
      "step = 20000: loss = 200.77127075195312  AVG RETURN: 500.000000\n",
      "step = 20050: loss = 140.58827209472656  AVG RETURN: 500.000000\n",
      "step = 20100: loss = 214.9564666748047  AVG RETURN: 500.000000\n",
      "step = 20150: loss = 197.02139282226562  AVG RETURN: 500.000000\n",
      "step = 20200: loss = 141.2690887451172  AVG RETURN: 488.899994\n",
      "step = 20250: loss = 95.46530151367188  AVG RETURN: 494.899994\n",
      "step = 20300: loss = 116.48043060302734  AVG RETURN: 493.899994\n",
      "step = 20350: loss = 140.99264526367188  AVG RETURN: 500.000000\n",
      "step = 20400: loss = 136.35595703125  AVG RETURN: 500.000000\n",
      "step = 20450: loss = 148.3652801513672  AVG RETURN: 500.000000\n",
      "step = 20500: loss = 197.9960174560547  AVG RETURN: 500.000000\n",
      "step = 20550: loss = 121.18470001220703  AVG RETURN: 500.000000\n",
      "step = 20600: loss = 132.4403839111328  AVG RETURN: 500.000000\n",
      "step = 20650: loss = 150.2322235107422  AVG RETURN: 500.000000\n",
      "step = 20700: loss = 110.94065856933594  AVG RETURN: 500.000000\n",
      "step = 20750: loss = 119.84187316894531  AVG RETURN: 500.000000\n",
      "step = 20800: loss = 142.40098571777344  AVG RETURN: 500.000000\n",
      "step = 20850: loss = 111.71942138671875  AVG RETURN: 500.000000\n",
      "step = 20900: loss = 195.6647186279297  AVG RETURN: 500.000000\n",
      "step = 20950: loss = 137.82228088378906  AVG RETURN: 500.000000\n",
      "step = 21000: loss = 128.4937744140625\n",
      "step = 21000: loss = 128.4937744140625  AVG RETURN: 500.000000\n",
      "step = 21050: loss = 102.31663513183594  AVG RETURN: 500.000000\n",
      "step = 21100: loss = 142.9320526123047  AVG RETURN: 500.000000\n",
      "step = 21150: loss = 250.15016174316406  AVG RETURN: 500.000000\n",
      "step = 21200: loss = 224.16720581054688  AVG RETURN: 500.000000\n",
      "step = 21250: loss = 192.65786743164062  AVG RETURN: 500.000000\n",
      "step = 21300: loss = 183.157958984375  AVG RETURN: 498.000000\n",
      "step = 21350: loss = 175.50157165527344  AVG RETURN: 488.399994\n",
      "step = 21400: loss = 194.4971466064453  AVG RETURN: 500.000000\n",
      "step = 21450: loss = 158.9686279296875  AVG RETURN: 500.000000\n",
      "step = 21500: loss = 171.51417541503906  AVG RETURN: 500.000000\n",
      "step = 21550: loss = 135.17420959472656  AVG RETURN: 500.000000\n",
      "step = 21600: loss = 202.6842041015625  AVG RETURN: 500.000000\n",
      "step = 21650: loss = 259.1453857421875  AVG RETURN: 500.000000\n",
      "step = 21700: loss = 209.42112731933594  AVG RETURN: 500.000000\n",
      "step = 21750: loss = 175.58267211914062  AVG RETURN: 500.000000\n",
      "step = 21800: loss = 186.2447509765625  AVG RETURN: 500.000000\n",
      "step = 21850: loss = 174.01966857910156  AVG RETURN: 500.000000\n",
      "step = 21900: loss = 195.92051696777344  AVG RETURN: 500.000000\n",
      "step = 21950: loss = 155.86141967773438  AVG RETURN: 500.000000\n",
      "step = 22000: loss = 148.4574737548828\n",
      "step = 22000: loss = 148.4574737548828  AVG RETURN: 500.000000\n",
      "step = 22050: loss = 132.37689208984375  AVG RETURN: 498.100006\n",
      "step = 22100: loss = 162.6877899169922  AVG RETURN: 500.000000\n",
      "step = 22150: loss = 158.81460571289062  AVG RETURN: 500.000000\n",
      "step = 22200: loss = 127.55394744873047  AVG RETURN: 500.000000\n",
      "step = 22250: loss = 174.73318481445312  AVG RETURN: 500.000000\n",
      "step = 22300: loss = 136.4491729736328  AVG RETURN: 500.000000\n",
      "step = 22350: loss = 207.2806854248047  AVG RETURN: 500.000000\n",
      "step = 22400: loss = 211.57952880859375  AVG RETURN: 500.000000\n",
      "step = 22450: loss = 124.59358215332031  AVG RETURN: 500.000000\n",
      "step = 22500: loss = 173.18829345703125  AVG RETURN: 500.000000\n",
      "step = 22550: loss = 221.15443420410156  AVG RETURN: 500.000000\n",
      "step = 22600: loss = 156.78335571289062  AVG RETURN: 500.000000\n",
      "step = 22650: loss = 124.56282043457031  AVG RETURN: 500.000000\n",
      "step = 22700: loss = 136.60638427734375  AVG RETURN: 500.000000\n",
      "step = 22750: loss = 199.0693817138672  AVG RETURN: 500.000000\n",
      "step = 22800: loss = 139.59713745117188  AVG RETURN: 500.000000\n",
      "step = 22850: loss = 177.2462921142578  AVG RETURN: 500.000000\n",
      "step = 22900: loss = 152.64320373535156  AVG RETURN: 500.000000\n",
      "step = 22950: loss = 188.2813720703125  AVG RETURN: 500.000000\n",
      "step = 23000: loss = 114.95697021484375\n",
      "step = 23000: loss = 114.95697021484375  AVG RETURN: 500.000000\n",
      "step = 23050: loss = 147.01426696777344  AVG RETURN: 500.000000\n",
      "step = 23100: loss = 135.69483947753906  AVG RETURN: 500.000000\n",
      "step = 23150: loss = 185.37225341796875  AVG RETURN: 500.000000\n",
      "step = 23200: loss = 200.15914916992188  AVG RETURN: 500.000000\n",
      "step = 23250: loss = 131.70504760742188  AVG RETURN: 500.000000\n",
      "step = 23300: loss = 128.41470336914062  AVG RETURN: 500.000000\n",
      "step = 23350: loss = 192.3489990234375  AVG RETURN: 500.000000\n",
      "step = 23400: loss = 113.93929290771484  AVG RETURN: 500.000000\n",
      "step = 23450: loss = 180.86175537109375  AVG RETURN: 500.000000\n",
      "step = 23500: loss = 193.4403076171875  AVG RETURN: 500.000000\n",
      "step = 23550: loss = 220.57725524902344  AVG RETURN: 500.000000\n",
      "step = 23600: loss = 276.388916015625  AVG RETURN: 500.000000\n",
      "step = 23650: loss = 294.1517333984375  AVG RETURN: 500.000000\n",
      "step = 23700: loss = 137.8287353515625  AVG RETURN: 500.000000\n",
      "step = 23750: loss = 191.4445037841797  AVG RETURN: 499.600006\n",
      "step = 23800: loss = 123.65955352783203  AVG RETURN: 500.000000\n",
      "step = 23850: loss = 134.68295288085938  AVG RETURN: 500.000000\n",
      "step = 23900: loss = 156.11669921875  AVG RETURN: 500.000000\n",
      "step = 23950: loss = 152.15455627441406  AVG RETURN: 500.000000\n",
      "step = 24000: loss = 260.8763122558594\n",
      "step = 24000: loss = 260.8763122558594  AVG RETURN: 500.000000\n",
      "step = 24050: loss = 125.45057678222656  AVG RETURN: 500.000000\n",
      "step = 24100: loss = 164.28684997558594  AVG RETURN: 500.000000\n",
      "step = 24150: loss = 183.63134765625  AVG RETURN: 491.700012\n",
      "step = 24200: loss = 177.5082244873047  AVG RETURN: 500.000000\n",
      "step = 24250: loss = 119.14147186279297  AVG RETURN: 491.000000\n",
      "step = 24300: loss = 122.5959701538086  AVG RETURN: 493.000000\n",
      "step = 24350: loss = 202.52328491210938  AVG RETURN: 500.000000\n",
      "step = 24400: loss = 139.37802124023438  AVG RETURN: 471.500000\n",
      "step = 24450: loss = 149.82044982910156  AVG RETURN: 490.500000\n",
      "step = 24500: loss = 301.6358337402344  AVG RETURN: 494.700012\n",
      "step = 24550: loss = 155.77090454101562  AVG RETURN: 500.000000\n",
      "step = 24600: loss = 124.32195281982422  AVG RETURN: 500.000000\n",
      "step = 24650: loss = 129.69895935058594  AVG RETURN: 500.000000\n",
      "step = 24700: loss = 173.01315307617188  AVG RETURN: 500.000000\n",
      "step = 24750: loss = 200.8062744140625  AVG RETURN: 500.000000\n",
      "step = 24800: loss = 194.79859924316406  AVG RETURN: 500.000000\n",
      "step = 24850: loss = 312.9483642578125  AVG RETURN: 500.000000\n",
      "step = 24900: loss = 291.5993957519531  AVG RETURN: 500.000000\n",
      "step = 24950: loss = 221.34307861328125  AVG RETURN: 500.000000\n",
      "step = 25000: loss = 142.79708862304688\n",
      "step = 25000: loss = 142.79708862304688  AVG RETURN: 500.000000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "num_iterations = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "eval_interval = 10\n",
    "num_eval_episodes = 10\n",
    "\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent_ppo.train = common.function(agent_ppo.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent_ppo.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent_ppo.collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "returns\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent_ppo.train(experience).loss\n",
    "  # experience, unused_info = next(iterator)\n",
    "  # train_loss = agent_ppo.train(experience).loss\n",
    "\n",
    "\n",
    "  step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "    print('step = {0}: loss = {1}  AVG RETURN: {2:2f}'.format(step, train_loss, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(459.8899871826172, 510.0)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, 2)\n",
    "# steps = range(0, num_iterations + 1, 10)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=max(returns) + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
