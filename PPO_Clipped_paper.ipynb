{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in ./venvIntelliJ/lib/python3.9/site-packages (2.22.4)\r\n",
      "Requirement already satisfied: numpy in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (1.23.5)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (9.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./venvIntelliJ/lib/python3.9/site-packages (3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-agents in ./venvIntelliJ/lib/python3.9/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: gin-config>=0.4.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.5.0)\r\n",
      "Requirement already satisfied: pygame==2.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.14.1)\r\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.23.0)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Requirement already satisfied: absl-py>=0.6.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.3.0)\r\n",
      "Requirement already satisfied: protobuf>=3.11.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (3.19.6)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (4.4.0)\r\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.19.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.2.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\r\n",
      "Requirement already satisfied: dm-tree in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyglet in ./venvIntelliJ/lib/python3.9/site-packages (2.0.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venvIntelliJ/lib/python3.9/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.0.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (22.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (9.3.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (4.38.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: numpy>=1.19 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venvIntelliJ/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./venvIntelliJ/lib/python3.9/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (14.0.6)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.12.6)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (4.4.0)\r\n",
      "Requirement already satisfied: packaging in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.28.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: setuptools in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (60.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.19.6)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.51.1)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.23.5)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvIntelliJ/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venvIntelliJ/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.13)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venvIntelliJ/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 21:13:49.893345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "# from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.networks import lstm_encoding_network\n",
    "\n",
    "\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "# env_name = \"Acrobot-v1\"\n",
    "# env_name =  \"Pendulum-v1\"\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_tensor_spec = tensor_spec.from_spec(train_env.observation_spec())\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "time_step_tensor_spec = tensor_spec.from_spec(train_env.time_step_spec())\n",
    "\n",
    "print('Observation: {0}'.format(observation_tensor_spec))\n",
    "# print('Action: {0}'.format(action_tensor_spec))\n",
    "# print('TimeStep: {0}'.format(time_step_tensor_spec))\n",
    "# print('TimeStep: {0}'.format(eval_env.current_time_step()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor_fc_layers = (64,64)\n",
    "value_fc_layers = (64,64)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    fc_layer_params=actor_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "    # seed_stream_class=tfp.util.SeedStream(seed=None, salt='tf_agents_sequential_layers')\n",
    "    seed_stream_class=tfp.util.SeedStream\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print(observation_tensor_spec)\n",
    "print(action_tensor_spec)\n",
    "print(time_step_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 21:13:54.818308: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 3e-4\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "# epsilon = 0.2\n",
    "epsilon = 0.1\n",
    "discount = 0.99\n",
    "lambda_value = 0.95\n",
    "train_step_counter = tf.Variable(0)\n",
    "# \n",
    "# agent_ppo = ppo_agent.PPOAgent(\n",
    "#     time_step_spec=time_step_tensor_spec,\n",
    "#     action_spec=action_tensor_spec,\n",
    "#     optimizer=optimizer,\n",
    "#     actor_net=actor_net,\n",
    "#     value_net=value_net,\n",
    "#     greedy_eval=False,\n",
    "#     lambda_value=lambda_value,\n",
    "#     discount_factor=discount,\n",
    "#     importance_ratio_clipping=epsilon,\n",
    "#     num_epochs=3,\n",
    "#     # use_gae=True,\n",
    "#     # log_prob_clipping=epsilon,\n",
    "#     # gradient_clipping=epsilon,\n",
    "#     # value_clipping=epsilon,\n",
    "#     train_step_counter=train_step_counter,\n",
    "# )\n",
    "# agent_ppo.initialize()\n",
    "\n",
    "# learning_rate = 3e-4\n",
    "# learning_rate = 1e-3\n",
    "# optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "# \n",
    "initial_learning_rate = 1e-3\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.96\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False) Uset the same trin_step_counter\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# epsilon = 0.2\n",
    "epsilon = 0.2\n",
    "discount = 0.99\n",
    "lambda_value = 0.95\n",
    "\n",
    "agent_ppo = ppo_clip_agent.PPOClipAgent(\n",
    "    time_step_spec=time_step_tensor_spec,\n",
    "    action_spec=action_tensor_spec,\n",
    "    optimizer=optimizer,\n",
    "    normalize_observations=True,\n",
    "    normalize_rewards=True,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    importance_ratio_clipping=epsilon,\n",
    "    num_epochs=3,\n",
    "    use_gae=True,\n",
    "    # log_prob_clipping=epsilon,\n",
    "    # gradient_clipping=epsilon,\n",
    "    # value_clipping=epsilon,\n",
    "    train_step_counter=train_step_counter,\n",
    ")\n",
    "agent_ppo.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_env.current_time_step()\n",
    "agent_ppo.policy.action(train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5\n",
      "16.5\n",
      "12.5\n",
      "22.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    info_spec=time_step_tensor_spec)\n",
    "\n",
    "num_eval_episodes = 2\n",
    "def evaluate_policy(policy, r=1):\n",
    "  for _ in range(r):\n",
    "    avg= compute_avg_return(eval_env, policy, num_eval_episodes)\n",
    "    print(avg)\n",
    "\n",
    "evaluate_policy(random_policy,5)\n",
    "\n",
    "# evaluate_policy(agent_ppo.policy)\n",
    "\n",
    "# evaluate_policy(agent_ppo.collect_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(\n{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03216462, -0.02890213,  0.04838202, -0.01524434]],\n      dtype=float32)>,\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent_ppo.collect_policy.trajectory_spec\n",
    "train_env.current_time_step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent_ppo.collect_policy.trajectory_spec,\n",
    "    batch_size=1,\n",
    "    max_length=10000)\n",
    "    # max_length=10000)\n",
    "\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "    # print(next_time_step.reward)\n",
    "\n",
    "initial_collect_steps = 10\n",
    "n_step_update = 10\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "batch_size = 64\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(batch_size)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# print(f'TimeStep: {train_env.time_step_spec()}')\n",
    "# print(f'Action: {train_env.action_spec()}')\n",
    "# print(f'Agent: {agent_ppo.collect_policy.time_step_spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Sample a batch of data from the buffer and update the agent's network.\u001B[39;00m\n\u001B[1;32m     28\u001B[0m experience, unused_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m---> 29\u001B[0m \u001B[43magent_ppo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# train_loss  = agent_ppo.train(experience).loss\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# experience, unused_info = next(iterator)\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# train_loss = agent_ppo.train(experience).loss\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (train_step_counter\u001B[38;5;241m.\u001B[39mvalue() \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1020\u001B[39m):\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "num_iterations = 10000\n",
    "collect_steps_per_iteration = 1\n",
    "eval_interval = 10\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent_ppo.train = common.function(agent_ppo.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent_ppo.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent_ppo.collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "returns\n",
    "new_epsilon = epsilon\n",
    "for _ in range(num_iterations):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  agent_ppo.train(experience)\n",
    "  # train_loss  = agent_ppo.train(experience).loss\n",
    "  # experience, unused_info = next(iterator)\n",
    "  # train_loss = agent_ppo.train(experience).loss\n",
    "  if not (train_step_counter.value() % 1020):\n",
    "      new_epsilon = new_epsilon - 0.01\n",
    "      if new_epsilon < 0.0:\n",
    "          new_epsilon = 0.0\n",
    "      agent_ppo._importance_ratio_clipping = new_epsilon\n",
    "      print('**** new_epsilon = {0} ***'.format(new_epsilon))\n",
    "\n",
    "\n",
    "  step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "    print('step = {0}: loss = {1}  AVG RETURN: {2:2f}  Epsilon: {3}'.format(step, train_loss, avg_return, agent_ppo._importance_ratio_clipping))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, 10)\n",
    "# steps = range(0, num_iterations + 1, 10)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=max(returns) + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tf_agents.environments.wrappers.TimeLimit"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "type(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
