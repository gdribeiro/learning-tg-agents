{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\r\n",
      "  Downloading imageio-2.22.4-py3-none-any.whl (3.4 MB)\r\n",
      "     |████████████████████████████████| 3.4 MB 2.9 MB/s            \r\n",
      "\u001B[?25hCollecting pillow>=8.3.2\r\n",
      "  Downloading Pillow-9.3.0-cp39-cp39-macosx_10_10_x86_64.whl (3.3 MB)\r\n",
      "     |████████████████████████████████| 3.3 MB 44.3 MB/s            \r\n",
      "\u001B[?25hCollecting numpy\r\n",
      "  Downloading numpy-1.23.5-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\r\n",
      "     |████████████████████████████████| 18.1 MB 10.8 MB/s            \r\n",
      "\u001B[?25hInstalling collected packages: pillow, numpy, imageio\r\n",
      "Successfully installed imageio-2.22.4 numpy-1.23.5 pillow-9.3.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyvirtualdisplay\r\n",
      "  Using cached PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: pyvirtualdisplay\r\n",
      "Successfully installed pyvirtualdisplay-3.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tf-agents\r\n",
      "  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\r\n",
      "     |████████████████████████████████| 1.4 MB 2.9 MB/s            \r\n",
      "\u001B[?25hCollecting absl-py>=0.6.1\r\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\r\n",
      "     |████████████████████████████████| 124 kB 60.1 MB/s            \r\n",
      "\u001B[?25hCollecting tensorflow-probability>=0.18.0\r\n",
      "  Downloading tensorflow_probability-0.19.0-py2.py3-none-any.whl (6.7 MB)\r\n",
      "     |████████████████████████████████| 6.7 MB 53.1 MB/s            \r\n",
      "\u001B[?25hRequirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Collecting typing-extensions>=3.7.4.3\r\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\r\n",
      "Collecting wrapt>=1.11.1\r\n",
      "  Using cached wrapt-1.14.1-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\r\n",
      "Collecting pygame==2.1.0\r\n",
      "  Using cached pygame-2.1.0-cp39-cp39-macosx_10_9_x86_64.whl (5.2 MB)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Collecting protobuf>=3.11.3\r\n",
      "  Downloading protobuf-4.21.11-cp37-abi3-macosx_10_9_universal2.whl (486 kB)\r\n",
      "     |████████████████████████████████| 486 kB 46.2 MB/s            \r\n",
      "\u001B[?25hCollecting gin-config>=0.4.0\r\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\r\n",
      "Collecting gym<=0.23.0,>=0.17.0\r\n",
      "  Using cached gym-0.23.0-py3-none-any.whl\r\n",
      "Collecting cloudpickle>=1.3\r\n",
      "  Using cached cloudpickle-2.2.0-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Collecting gym-notices>=0.0.4\r\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Collecting dm-tree\r\n",
      "  Using cached dm_tree-0.1.7-cp39-cp39-macosx_10_9_x86_64.whl (109 kB)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Collecting gast>=0.3.2\r\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "Installing collected packages: gym-notices, gast, dm-tree, cloudpickle, absl-py, wrapt, typing-extensions, tensorflow-probability, pygame, protobuf, gym, gin-config, tf-agents\r\n",
      "Successfully installed absl-py-1.3.0 cloudpickle-2.2.0 dm-tree-0.1.7 gast-0.5.3 gin-config-0.5.0 gym-0.23.0 gym-notices-0.0.8 protobuf-4.21.11 pygame-2.1.0 tensorflow-probability-0.19.0 tf-agents-0.15.0 typing-extensions-4.4.0 wrapt-1.14.1\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyglet\r\n",
      "  Downloading pyglet-2.0.1-py3-none-any.whl (964 kB)\r\n",
      "     |████████████████████████████████| 964 kB 2.8 MB/s            \r\n",
      "\u001B[?25hInstalling collected packages: pyglet\r\n",
      "Successfully installed pyglet-2.0.1\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents as tfa\n",
    "\n",
    "# Seed for PPO actor network\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# PPO Agent\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.agents.ppo import ppo_actor_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "# old agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# register(\n",
    "#     id='CartPole-v1',\n",
    "#     entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "#     max_episode_steps=500,\n",
    "#     reward_threshold=475.0,\n",
    "# )\n",
    "\n",
    "env_name = \"CartPole-v1\" # @param {type:\"string\"}\n",
    "num_iterations = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (64,64)\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 3e-4  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "\n",
    "actor_fc_layers = (8,16,32,64,128,64,32,16,8,4,2)\n",
    "value_fc_layers = (8,16,32,64,128,64,128,64,32,16,8,4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_tensor_spec = tensor_spec.from_spec(train_env.observation_spec())\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "time_step_tensor_spec = tensor_spec.from_spec(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "Action: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TimeStep: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('Observation: {0}'.format(observation_tensor_spec))\n",
    "print('Action: {0}'.format(action_tensor_spec))\n",
    "print('TimeStep: {0}'.format(time_step_tensor_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netowork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_net_builder = ppo_actor_network.PPOActorNetwork()\n",
    "# actor_net = actor_net_builder.create_sequential_actor_net(\n",
    "#     actor_fc_layers, action_tensor_spec)\n",
    "\n",
    "\n",
    "actor_fc_layers = (64,64)\n",
    "value_fc_layers = (64,64)\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_tensor_spec,\n",
    "        action_tensor_spec,\n",
    "        fc_layer_params=actor_fc_layers,\n",
    "        activation_fn=tf.nn.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.Orthogonal(seed=1),\n",
    "        # seed_stream_class=DeterministicSeedStream,\n",
    "        seed_stream_class=tfp.util.SeedStream\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n",
      "TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print(observation_tensor_spec)\n",
    "print(action_tensor_spec)\n",
    "print(time_step_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent_ppo = ppo_agent.PPOAgent(\n",
    "    time_step_spec=time_step_tensor_spec,\n",
    "    action_spec=action_tensor_spec,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter\n",
    "    # compute_value_and_advantage_in_train=True,\n",
    "    # update_normalizers_in_train=False,\n",
    "    # num_epochs=10\n",
    ")\n",
    "agent_ppo.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-0.01432419, -0.0279336 ,  0.04301443,  0.01094299]],\n",
       "      dtype=float32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.4\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    info_spec=time_step_tensor_spec)\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "  for _ in range(1):\n",
    "    avg= compute_avg_return(eval_env, policy, num_eval_episodes)\n",
    "    print(avg)\n",
    "\n",
    "# evaluate_policy(random_policy)\n",
    "\n",
    "# evaluate_policy(agent_ppo.policy)\n",
    "\n",
    "evaluate_policy(agent_ppo.collect_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'policy_info': {'dist_params': {'logits': TensorSpec(shape=(2,), dtype=tf.float32, name='CategoricalProjectionNetwork_logits')}},\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "a = np.array(1)\n",
    "\n",
    "# print(a.shape)\n",
    "data_spec =  (\n",
    "        tf.TensorSpec([3], tf.float32, 'action'),\n",
    "        (\n",
    "            tf.TensorSpec([5], tf.float32, 'lidar'),\n",
    "            tf.TensorSpec([3, 2], tf.float32, 'camera')\n",
    "        )\n",
    ")\n",
    "# print(data_spec)\n",
    "print(agent_ppo.collect_policy.trajectory_spec)\n",
    "print(agent_ppo.policy.trajectory_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_ppo.policy.trajectory_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The two structures do not match:\n  [Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .})]\nvs.\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': (),\n 'reward': .,\n 'step_type': .})\nValues:\n  [Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04298007,  0.24571925, -0.1220688 , -0.9009734 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08508154, 0.00266403]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03806568,  0.05244385, -0.14008826, -0.6490168 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17216936, 0.03608777]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03701681, -0.14047737, -0.1530686 , -0.40352118]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33580795, -0.01110766]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03982635, -0.33313474, -0.16113903, -0.16274066]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.3494506 , -0.01963349]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04648905, -0.13611685, -0.16439384, -0.5016091 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33127463, -0.01014704]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04921139, -0.32858637, -0.17442602, -0.26491156]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.34788427, -0.02377561]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.05578311, -0.5208454 , -0.17972425, -0.03192051]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.2813996 , -0.00610545]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.06620003, -0.32366234, -0.18036266, -0.37548605]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.3432741 , -0.03064866]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.07267327, -0.51582557, -0.18787238, -0.14465651]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33721018, -0.03080483]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.08298978, -0.31857947, -0.19076551, -0.4902296 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33662078, -0.03686972]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.08936137, -0.12135085, -0.2005701 , -0.8364513 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.301283  , -0.02113277]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.09178839,  0.07586229, -0.21729913, -1.1849138 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14220205, 0.0218972 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01472262, -0.04345044, -0.02352989, -0.01178013]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14553243, 0.00943403]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01385361, -0.23822717, -0.0237655 ,  0.27338696]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.02381912, 0.01475563]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00908907, -0.4330021 , -0.01829776,  0.55848044]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01569436, 0.01037242]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 4.2902524e-04, -6.2786251e-01, -7.1281502e-03,  8.4534287e-01]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00053594,  0.00186943]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01212822, -0.432644  ,  0.00977871,  0.5504269 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02461435, -0.01134331]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02078111, -0.23766078,  0.02078724,  0.26084092]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04015506, -0.02064495]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02553432, -0.04284164,  0.02600406, -0.02521363]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.06284717, -0.07925381]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02639115, -0.23832667,  0.02549979,  0.27555913]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04773846, -0.02602965]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03115769, -0.04357764,  0.03101097, -0.00897334]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.02580372, -0.0851023 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03202924,  0.15108617,  0.03083151, -0.29171288]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03368802, -0.0309953 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02900752, -0.04446152,  0.02499725,  0.01053227]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00479399, -0.07246946]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02989675,  0.15029319,  0.02520789, -0.2741601 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02630478, -0.02886846]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02689088,  0.34504658,  0.01972469, -0.55878705]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02524156, -0.02402964]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01998995,  0.5398862 ,  0.00854895, -0.8451909 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01036837, -0.01687713]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00919223,  0.73489046, -0.00835487, -1.1351732 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.0121792 , -0.00500205]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00550558,  0.9301207 , -0.03105833, -1.4304647 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.04158866, 0.01220847]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02410799,  0.7353956 , -0.05966762, -1.1476476 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07675771, 0.03479118]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03881591,  0.9312437 , -0.08262058, -1.4584289 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.10339123, 0.05250373]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.05744078,  1.1272762 , -0.11178915, -1.7757374 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.13485384, 0.07341778]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0799863 ,  0.93357736, -0.14730391, -1.5198003 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16773805, 0.09542407]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.09865785,  0.7405111 , -0.17769991, -1.2764881 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17942624, 0.10795796]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.11346807,  0.5480434 , -0.20322967, -1.0443019 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18276398, 0.11556946]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.12442894,  0.7451979 , -0.22411571, -1.3932866 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18482411, 0.12090024]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0042406 , -0.02996069,  0.03141524, -0.0321372 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.07329048, -0.0428854 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00364138, -0.22551873,  0.03077249,  0.2702897 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03645298, -0.00500262]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00086899, -0.03084912,  0.03617829, -0.0125308 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.03407113, -0.04756133]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00148597, -0.22647074,  0.03592767,  0.29134372]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04385035, -0.01044536]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00601539, -0.031879  ,  0.04175455,  0.01020492]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01370441, -0.05046679]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00665297,  0.16262004,  0.04195864, -0.26901707]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04054052, -0.01219287]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00340057,  0.35711887,  0.03657831, -0.5481763 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03318108, -0.00928431]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00374181,  0.16150266,  0.02561478, -0.24419665]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01762611, -0.00264996]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00697186, -0.03397561,  0.02073085,  0.05645449]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03716829, -0.00187856]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00629235,  0.16084306,  0.02185993, -0.22961628]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0121863 , -0.00019786]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00950921, -0.03458435,  0.01726761,  0.06988108]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04766508,  0.00757584]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00881752, -0.22994955,  0.01866523,  0.36796162]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02244724,  0.00222464]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00421853, -0.42533168,  0.02602446,  0.666471  ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03177748, -0.00328049]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.0042881 , -0.62080574,  0.03935388,  0.9672331 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0484155 , -0.01359964]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01670421, -0.4262337 ,  0.05869855,  0.6871682 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.07186096, -0.02887082]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02522889, -0.23197354,  0.07244191,  0.41352722]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0878727 , -0.03946423]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02986836, -0.03794918,  0.08071245,  0.14453329]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.17785661, -0.06319099]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03062734,  0.15592968,  0.08360312, -0.12163454]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.13593403, -0.03282166]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02750875,  0.34976053,  0.08117043, -0.38681462]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.087263  , -0.02837742]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02051354,  0.15358578,  0.07343414, -0.06968346]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.19717541, -0.02371853]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01744182,  0.3475823 ,  0.07204047, -0.33832374]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.07574078, -0.02161871]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01049018,  0.54160917,  0.06527399, -0.6074475 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.06663577, -0.01685732]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 3.4200485e-04,  7.3576069e-01,  5.3125042e-02, -8.7887770e-01]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04957637, -0.00899223]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01505722,  0.9301221 ,  0.03554749, -1.1543971 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02306185,  0.0028465 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03365966,  1.1247629 ,  0.01245955, -1.4357251 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01312432, 0.01983589]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.05615492,  0.9294895 , -0.01625496, -1.1391748 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0561183 , 0.04242103]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.07474471,  1.1248202 , -0.03903845, -1.4369109 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08743115, 0.06079208]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.09724111,  0.93020064, -0.06777667, -1.1566783 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.12174427, 0.08191777]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.11584512,  1.1261375 , -0.09091023, -1.4698198 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14569248, 0.09768341]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.13836788,  0.9322377 , -0.12030663, -1.2068621 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16993819, 0.11623067]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.15701263,  0.7388578 , -0.14444387, -0.95417374]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18467763, 0.1315525 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.17178978,  0.93559647, -0.16352734, -1.2885275 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.19050172,  1.1323768 , -0.1892979 , -1.6276232 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.21314925,  1.329153  , -0.22185037, -1.9728366 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00469154, -0.04090569,  0.00232736,  0.00710431]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.06192306, -0.0268805 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00550965, -0.23606093,  0.00246945,  0.30052063]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01365832, -0.00466931]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01023087, -0.431218  ,  0.00847986,  0.5939813 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02229353, -0.0096255 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01885523, -0.62645763,  0.02035949,  0.8893233 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03869184, -0.01913819]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03138438, -0.8218498 ,  0.03814595,  1.1883361 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.06213802, -0.03344876]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04782138, -1.017445  ,  0.06191267,  1.4927276 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.09153353, -0.05232438]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.06817028, -1.2132632 ,  0.09176723,  1.8040829 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.12525114, -0.074455  ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.09243555, -1.4092824 ,  0.1278489 ,  2.1238163 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.16002262, -0.09777012]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.12062119, -1.6054236 ,  0.1703252 ,  2.4531121 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.1841426 , -0.11907272]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.15272966, -1.4121087 ,  0.21938746,  2.2171695 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.18907121, -0.13389906]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01079242,  0.02660162, -0.01473441, -0.01773535]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.00549602, 0.03408786]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01132446, -0.16830596, -0.01508912,  0.27026254]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01326879, 0.01145379]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00795834,  0.02702803, -0.00968387, -0.02714114]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.00897965, 0.02298019]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0084989 ,  0.2222875 , -0.01022669, -0.32286364]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.02314531, 0.00881705]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01294465,  0.02731267, -0.01668396, -0.03342325]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03283613, 0.03257173]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0134909 ,  0.22266985, -0.01735243, -0.3313231 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03287001, 0.01439362]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0179443 ,  0.41803443, -0.02397889, -0.62942725]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.041667  , 0.01952369]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02630499,  0.22325519, -0.03656743, -0.34439144]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0579144 , 0.02934208]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03077009,  0.41887772, -0.04345526, -0.64837754]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.06651624, 0.03467448]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03914765,  0.22438724, -0.05642281, -0.36968905]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08226874, 0.04467025]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.04363539,  0.03011046, -0.06381659, -0.09531711]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16921838, 0.08028903]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0442376 , -0.16404155, -0.06572293,  0.17656872]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07834936, 0.04035993]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.04095677, -0.35816437, -0.06219156,  0.4478157 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07404139, 0.03785327]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03379348, -0.16222034, -0.05323525,  0.13619532]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.09180444, 0.03276888]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03054907, -0.35654098, -0.05051134,  0.41161954]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0590394 , 0.02973531]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02341825, -0.16074072, -0.04227895,  0.10344974]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.12338265, 0.02318555]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02020344, -0.35523206, -0.04020996,  0.38249975]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.04431962, 0.02143039]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0130988 , -0.15956295, -0.03255996,  0.07741477]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14760302, 0.00953922]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00990754, -0.35420337, -0.03101167,  0.35964972]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0299738, 0.01298  ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00282347, -0.54887104, -0.02381867,  0.6423949 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01879706, 0.00685172]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00815395, -0.74365306, -0.01097077,  0.92748296]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00073636, -0.00339631]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})]\nvs.\n  Trajectory(\n{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32)),\n 'policy_info': (),\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb Cell 21\u001B[0m in \u001B[0;36m<cell line: 27>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001B[0m     \u001B[39m# replay_buffer.add_batch(b_traj)\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001B[0m \u001B[39mfor\u001B[39;00m _ \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(initial_collect_steps):\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001B[0m     collect_step(train_env, agent_ppo\u001B[39m.\u001B[39;49mcollect_policy)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001B[0m \u001B[39m# This loop is so common in RL, that we provide standard implementations of\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001B[0m \u001B[39m# these. For more details see the drivers module.\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001B[0m \n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001B[0m \u001B[39m# Dataset generates trajectories with shape [BxTx...] where\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001B[0m \u001B[39m# T = n_step_update + 1.\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001B[0m dataset \u001B[39m=\u001B[39m replay_buffer\u001B[39m.\u001B[39mas_dataset(\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001B[0m     num_parallel_calls\u001B[39m=\u001B[39m\u001B[39m3\u001B[39m, sample_batch_size\u001B[39m=\u001B[39mbatch_size,\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001B[0m     num_steps\u001B[39m=\u001B[39mn_step_update \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m)\u001B[39m.\u001B[39mprefetch(\u001B[39m100\u001B[39m)\n",
      "\u001B[1;32m/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb Cell 21\u001B[0m in \u001B[0;36mcollect_step\u001B[0;34m(environment, policy)\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001B[0m     b_traj\u001B[39m.\u001B[39mappend(traj)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001B[0m values_batched \u001B[39m=\u001B[39m tf\u001B[39m.\u001B[39mnest\u001B[39m.\u001B[39mmap_structure(\u001B[39mlambda\u001B[39;00m t: tf\u001B[39m.\u001B[39mstack(t), b_traj)\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001B[0m replay_buffer\u001B[39m.\u001B[39;49madd_batch(values_batched)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tf_agents/replay_buffers/replay_buffer.py:83\u001B[0m, in \u001B[0;36mReplayBuffer.add_batch\u001B[0;34m(self, items)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39madd_batch\u001B[39m(\u001B[39mself\u001B[39m, items):\n\u001B[1;32m     73\u001B[0m   \u001B[39m\"\"\"Adds a batch of items to the replay buffer.\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \n\u001B[1;32m     75\u001B[0m \u001B[39m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[39m    Adds `items` to the replay buffer.\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[39m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_add_batch(items)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:193\u001B[0m, in \u001B[0;36mTFUniformReplayBuffer._add_batch\u001B[0;34m(self, items)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_add_batch\u001B[39m(\u001B[39mself\u001B[39m, items):\n\u001B[1;32m    182\u001B[0m   \u001B[39m\"\"\"Adds a batch of items to the replay buffer.\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \n\u001B[1;32m    184\u001B[0m \u001B[39m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[39m    ValueError: If called more than once.\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \u001B[39m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 193\u001B[0m   nest_utils\u001B[39m.\u001B[39;49massert_same_structure(items, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_data_spec)\n\u001B[1;32m    194\u001B[0m   \u001B[39m# Calling get_outer_rank here will validate that all items have the same\u001B[39;00m\n\u001B[1;32m    195\u001B[0m   \u001B[39m# outer rank. This was not usually an issue, but now that it's easier to\u001B[39;00m\n\u001B[1;32m    196\u001B[0m   \u001B[39m# call this from an eager context it's easy to make the mistake.\u001B[39;00m\n\u001B[1;32m    197\u001B[0m   nest_utils\u001B[39m.\u001B[39mget_outer_rank(\n\u001B[1;32m    198\u001B[0m       tf\u001B[39m.\u001B[39mnest\u001B[39m.\u001B[39mmap_structure(tf\u001B[39m.\u001B[39mconvert_to_tensor, items),\n\u001B[1;32m    199\u001B[0m       \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_data_spec)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tf_agents/utils/nest_utils.py:124\u001B[0m, in \u001B[0;36massert_same_structure\u001B[0;34m(nest1, nest2, check_types, expand_composites, allow_shallow_nest1, message)\u001B[0m\n\u001B[1;32m    120\u001B[0m str1 \u001B[39m=\u001B[39m tf\u001B[39m.\u001B[39mnest\u001B[39m.\u001B[39mmap_structure(\n\u001B[1;32m    121\u001B[0m     \u001B[39mlambda\u001B[39;00m _: _DOT, nest1, expand_composites\u001B[39m=\u001B[39mexpand_composites)\n\u001B[1;32m    122\u001B[0m str2 \u001B[39m=\u001B[39m tf\u001B[39m.\u001B[39mnest\u001B[39m.\u001B[39mmap_structure(\n\u001B[1;32m    123\u001B[0m     \u001B[39mlambda\u001B[39;00m _: _DOT, nest2, expand_composites\u001B[39m=\u001B[39mexpand_composites)\n\u001B[0;32m--> 124\u001B[0m \u001B[39mraise\u001B[39;00m exception(\u001B[39m'\u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m:\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m  \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39mvs.\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m  \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39mValues:\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m  \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39mvs.\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m  \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m'\u001B[39m\n\u001B[1;32m    125\u001B[0m                 \u001B[39m.\u001B[39mformat(message, str1, str2, nest1, nest2))\n",
      "\u001B[0;31mTypeError\u001B[0m: The two structures do not match:\n  [Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .}), Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': {'dist_params': {'logits': .}},\n 'reward': .,\n 'step_type': .})]\nvs.\n  Trajectory(\n{'action': .,\n 'discount': .,\n 'next_step_type': .,\n 'observation': .,\n 'policy_info': (),\n 'reward': .,\n 'step_type': .})\nValues:\n  [Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04298007,  0.24571925, -0.1220688 , -0.9009734 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08508154, 0.00266403]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03806568,  0.05244385, -0.14008826, -0.6490168 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17216936, 0.03608777]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03701681, -0.14047737, -0.1530686 , -0.40352118]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33580795, -0.01110766]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03982635, -0.33313474, -0.16113903, -0.16274066]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.3494506 , -0.01963349]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04648905, -0.13611685, -0.16439384, -0.5016091 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33127463, -0.01014704]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04921139, -0.32858637, -0.17442602, -0.26491156]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.34788427, -0.02377561]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.05578311, -0.5208454 , -0.17972425, -0.03192051]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.2813996 , -0.00610545]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.06620003, -0.32366234, -0.18036266, -0.37548605]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.3432741 , -0.03064866]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.07267327, -0.51582557, -0.18787238, -0.14465651]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33721018, -0.03080483]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.08298978, -0.31857947, -0.19076551, -0.4902296 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.33662078, -0.03686972]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.08936137, -0.12135085, -0.2005701 , -0.8364513 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.301283  , -0.02113277]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.09178839,  0.07586229, -0.21729913, -1.1849138 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14220205, 0.0218972 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01472262, -0.04345044, -0.02352989, -0.01178013]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14553243, 0.00943403]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01385361, -0.23822717, -0.0237655 ,  0.27338696]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.02381912, 0.01475563]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00908907, -0.4330021 , -0.01829776,  0.55848044]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01569436, 0.01037242]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 4.2902524e-04, -6.2786251e-01, -7.1281502e-03,  8.4534287e-01]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00053594,  0.00186943]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01212822, -0.432644  ,  0.00977871,  0.5504269 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02461435, -0.01134331]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02078111, -0.23766078,  0.02078724,  0.26084092]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04015506, -0.02064495]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02553432, -0.04284164,  0.02600406, -0.02521363]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.06284717, -0.07925381]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02639115, -0.23832667,  0.02549979,  0.27555913]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04773846, -0.02602965]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03115769, -0.04357764,  0.03101097, -0.00897334]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.02580372, -0.0851023 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03202924,  0.15108617,  0.03083151, -0.29171288]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03368802, -0.0309953 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02900752, -0.04446152,  0.02499725,  0.01053227]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00479399, -0.07246946]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02989675,  0.15029319,  0.02520789, -0.2741601 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02630478, -0.02886846]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02689088,  0.34504658,  0.01972469, -0.55878705]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02524156, -0.02402964]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01998995,  0.5398862 ,  0.00854895, -0.8451909 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01036837, -0.01687713]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00919223,  0.73489046, -0.00835487, -1.1351732 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.0121792 , -0.00500205]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00550558,  0.9301207 , -0.03105833, -1.4304647 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.04158866, 0.01220847]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02410799,  0.7353956 , -0.05966762, -1.1476476 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07675771, 0.03479118]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03881591,  0.9312437 , -0.08262058, -1.4584289 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.10339123, 0.05250373]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.05744078,  1.1272762 , -0.11178915, -1.7757374 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.13485384, 0.07341778]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0799863 ,  0.93357736, -0.14730391, -1.5198003 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16773805, 0.09542407]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.09865785,  0.7405111 , -0.17769991, -1.2764881 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.17942624, 0.10795796]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.11346807,  0.5480434 , -0.20322967, -1.0443019 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18276398, 0.11556946]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.12442894,  0.7451979 , -0.22411571, -1.3932866 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18482411, 0.12090024]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0042406 , -0.02996069,  0.03141524, -0.0321372 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.07329048, -0.0428854 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00364138, -0.22551873,  0.03077249,  0.2702897 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03645298, -0.00500262]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00086899, -0.03084912,  0.03617829, -0.0125308 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.03407113, -0.04756133]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00148597, -0.22647074,  0.03592767,  0.29134372]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04385035, -0.01044536]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00601539, -0.031879  ,  0.04175455,  0.01020492]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01370441, -0.05046679]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00665297,  0.16262004,  0.04195864, -0.26901707]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04054052, -0.01219287]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00340057,  0.35711887,  0.03657831, -0.5481763 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03318108, -0.00928431]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00374181,  0.16150266,  0.02561478, -0.24419665]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01762611, -0.00264996]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00697186, -0.03397561,  0.02073085,  0.05645449]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03716829, -0.00187856]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00629235,  0.16084306,  0.02185993, -0.22961628]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0121863 , -0.00019786]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00950921, -0.03458435,  0.01726761,  0.06988108]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04766508,  0.00757584]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00881752, -0.22994955,  0.01866523,  0.36796162]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02244724,  0.00222464]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00421853, -0.42533168,  0.02602446,  0.666471  ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03177748, -0.00328049]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.0042881 , -0.62080574,  0.03935388,  0.9672331 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0484155 , -0.01359964]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01670421, -0.4262337 ,  0.05869855,  0.6871682 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.07186096, -0.02887082]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02522889, -0.23197354,  0.07244191,  0.41352722]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.0878727 , -0.03946423]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02986836, -0.03794918,  0.08071245,  0.14453329]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.17785661, -0.06319099]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03062734,  0.15592968,  0.08360312, -0.12163454]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.13593403, -0.03282166]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02750875,  0.34976053,  0.08117043, -0.38681462]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.087263  , -0.02837742]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.02051354,  0.15358578,  0.07343414, -0.06968346]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.19717541, -0.02371853]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01744182,  0.3475823 ,  0.07204047, -0.33832374]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.07574078, -0.02161871]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01049018,  0.54160917,  0.06527399, -0.6074475 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.06663577, -0.01685732]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 3.4200485e-04,  7.3576069e-01,  5.3125042e-02, -8.7887770e-01]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.04957637, -0.00899223]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01505722,  0.9301221 ,  0.03554749, -1.1543971 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02306185,  0.0028465 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03365966,  1.1247629 ,  0.01245955, -1.4357251 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01312432, 0.01983589]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.05615492,  0.9294895 , -0.01625496, -1.1391748 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0561183 , 0.04242103]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.07474471,  1.1248202 , -0.03903845, -1.4369109 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08743115, 0.06079208]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.09724111,  0.93020064, -0.06777667, -1.1566783 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.12174427, 0.08191777]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.11584512,  1.1261375 , -0.09091023, -1.4698198 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14569248, 0.09768341]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.13836788,  0.9322377 , -0.12030663, -1.2068621 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16993819, 0.11623067]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.15701263,  0.7388578 , -0.14444387, -0.95417374]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18467763, 0.1315525 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.17178978,  0.93559647, -0.16352734, -1.2885275 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.19050172,  1.1323768 , -0.1892979 , -1.6276232 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.21314925,  1.329153  , -0.22185037, -1.9728366 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.18977317, 0.13628699]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00469154, -0.04090569,  0.00232736,  0.00710431]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.06192306, -0.0268805 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00550965, -0.23606093,  0.00246945,  0.30052063]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01365832, -0.00466931]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01023087, -0.431218  ,  0.00847986,  0.5939813 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.02229353, -0.0096255 ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.01885523, -0.62645763,  0.02035949,  0.8893233 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.03869184, -0.01913819]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.03138438, -0.8218498 ,  0.03814595,  1.1883361 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.06213802, -0.03344876]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.04782138, -1.017445  ,  0.06191267,  1.4927276 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.09153353, -0.05232438]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.06817028, -1.2132632 ,  0.09176723,  1.8040829 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.12525114, -0.074455  ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.09243555, -1.4092824 ,  0.1278489 ,  2.1238163 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.16002262, -0.09777012]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.12062119, -1.6054236 ,  0.1703252 ,  2.4531121 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.1841426 , -0.11907272]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.15272966, -1.4121087 ,  0.21938746,  2.2171695 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.18907121, -0.13389906]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01079242,  0.02660162, -0.01473441, -0.01773535]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.00549602, 0.03408786]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01132446, -0.16830596, -0.01508912,  0.27026254]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01326879, 0.01145379]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00795834,  0.02702803, -0.00968387, -0.02714114]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.00897965, 0.02298019]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0084989 ,  0.2222875 , -0.01022669, -0.32286364]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.02314531, 0.00881705]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.01294465,  0.02731267, -0.01668396, -0.03342325]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03283613, 0.03257173]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0134909 ,  0.22266985, -0.01735243, -0.3313231 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03287001, 0.01439362]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0179443 ,  0.41803443, -0.02397889, -0.62942725]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.041667  , 0.01952369]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02630499,  0.22325519, -0.03656743, -0.34439144]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0579144 , 0.02934208]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03077009,  0.41887772, -0.04345526, -0.64837754]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.06651624, 0.03467448]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03914765,  0.22438724, -0.05642281, -0.36968905]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.08226874, 0.04467025]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.04363539,  0.03011046, -0.06381659, -0.09531711]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.16921838, 0.08028903]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0442376 , -0.16404155, -0.06572293,  0.17656872]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07834936, 0.04035993]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.04095677, -0.35816437, -0.06219156,  0.4478157 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07404139, 0.03785327]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03379348, -0.16222034, -0.05323525,  0.13619532]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.09180444, 0.03276888]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.03054907, -0.35654098, -0.05051134,  0.41161954]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0590394 , 0.02973531]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02341825, -0.16074072, -0.04227895,  0.10344974]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.12338265, 0.02318555]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.02020344, -0.35523206, -0.04020996,  0.38249975]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.04431962, 0.02143039]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.0130988 , -0.15956295, -0.03255996,  0.07741477]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.14760302, 0.00953922]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00990754, -0.35420337, -0.03101167,  0.35964972]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.0299738, 0.01298  ]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[ 0.00282347, -0.54887104, -0.02381867,  0.6423949 ]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.01879706, 0.00685172]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}), Trajectory(\n{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\narray([[-0.00815395, -0.74365306, -0.01097077,  0.92748296]],\n      dtype=float32)>,\n 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00073636, -0.00339631]], dtype=float32)>}},\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})]\nvs.\n  Trajectory(\n{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32)),\n 'policy_info': (),\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})."
     ]
    }
   ],
   "source": [
    "# ds = (64,)\n",
    "batch_size = 100\n",
    "\n",
    "initial_collect_steps=1\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent_ppo.policy.trajectory_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=100)\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    b_traj = []\n",
    "    for i in range(batch_size):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "        b_traj.append(traj)\n",
    "    values_batched = tf.nest.map_structure(lambda t: tf.stack(t), b_traj)\n",
    "    replay_buffer.add_batch(values_batched)\n",
    "\n",
    "    \n",
    "\n",
    "    # replay_buffer.add_batch(b_traj)\n",
    "\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(100)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "print(f'TimeStep: {train_env.time_step_spec()}')\n",
    "print(f'Action: {train_env.action_spec()}')\n",
    "print(f'Agent: {agent_ppo.collect_policy.time_step_spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [TFUniformReplayBuffer is empty. Make sure to add items before sampling the buffer.] [Condition x > y did not hold element-wise:] [x (TFUniformReplayBuffer/get_next/SelectV2_1:0) = ] [0] [y (TFUniformReplayBuffer/get_next/SelectV2:0) = ] [0]\n\t [[{{function_node TFUniformReplayBuffer_get_next_assert_greater_Assert_AssertGuard_false_86492}}{{node TFUniformReplayBuffer/get_next/assert_greater/Assert/AssertGuard/Assert}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb Cell 23\u001B[0m in \u001B[0;36m<cell line: 23>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001B[0m   collect_step(train_env, agent_ppo\u001B[39m.\u001B[39mcollect_policy)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001B[0m \u001B[39m# Sample a batch of data from the buffer and update the agent's network.\u001B[39;00m\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001B[0m experience, unused_info \u001B[39m=\u001B[39m \u001B[39mnext\u001B[39;49m(iterator)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001B[0m train_loss \u001B[39m=\u001B[39m agent_ppo\u001B[39m.\u001B[39mtrain(experience)\u001B[39m.\u001B[39mloss\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001B[0m \u001B[39m# experience, unused_info = next(iterator)\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/PPO_cartpole_batch_64.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001B[0m \u001B[39m# train_loss = agent_ppo.train(experience).loss\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:766\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__next__\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[1;32m    765\u001B[0m   \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 766\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_next_internal()\n\u001B[1;32m    767\u001B[0m   \u001B[39mexcept\u001B[39;00m errors\u001B[39m.\u001B[39mOutOfRangeError:\n\u001B[1;32m    768\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    746\u001B[0m \u001B[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[39m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[1;32m    748\u001B[0m \u001B[39mwith\u001B[39;00m context\u001B[39m.\u001B[39mexecution_mode(context\u001B[39m.\u001B[39mSYNC):\n\u001B[0;32m--> 749\u001B[0m   ret \u001B[39m=\u001B[39m gen_dataset_ops\u001B[39m.\u001B[39;49miterator_get_next(\n\u001B[1;32m    750\u001B[0m       \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_iterator_resource,\n\u001B[1;32m    751\u001B[0m       output_types\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_flat_output_types,\n\u001B[1;32m    752\u001B[0m       output_shapes\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_flat_output_shapes)\n\u001B[1;32m    754\u001B[0m   \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    755\u001B[0m     \u001B[39m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[1;32m    756\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_element_spec\u001B[39m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[39m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3017\u001B[0m, in \u001B[0;36miterator_get_next\u001B[0;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[1;32m   3015\u001B[0m   \u001B[39mreturn\u001B[39;00m _result\n\u001B[1;32m   3016\u001B[0m \u001B[39mexcept\u001B[39;00m _core\u001B[39m.\u001B[39m_NotOkStatusException \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m-> 3017\u001B[0m   _ops\u001B[39m.\u001B[39;49mraise_from_not_ok_status(e, name)\n\u001B[1;32m   3018\u001B[0m \u001B[39mexcept\u001B[39;00m _core\u001B[39m.\u001B[39m_FallbackException:\n\u001B[1;32m   3019\u001B[0m   \u001B[39mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7164\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   7162\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mraise_from_not_ok_status\u001B[39m(e, name):\n\u001B[1;32m   7163\u001B[0m   e\u001B[39m.\u001B[39mmessage \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m (\u001B[39m\"\u001B[39m\u001B[39m name: \u001B[39m\u001B[39m\"\u001B[39m \u001B[39m+\u001B[39m name \u001B[39mif\u001B[39;00m name \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m-> 7164\u001B[0m   \u001B[39mraise\u001B[39;00m core\u001B[39m.\u001B[39m_status_to_exception(e) \u001B[39mfrom\u001B[39;00m \u001B[39mNone\u001B[39m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: assertion failed: [TFUniformReplayBuffer is empty. Make sure to add items before sampling the buffer.] [Condition x > y did not hold element-wise:] [x (TFUniformReplayBuffer/get_next/SelectV2_1:0) = ] [0] [y (TFUniformReplayBuffer/get_next/SelectV2:0) = ] [0]\n\t [[{{function_node TFUniformReplayBuffer_get_next_assert_greater_Assert_AssertGuard_false_86492}}{{node TFUniformReplayBuffer/get_next/assert_greater/Assert/AssertGuard/Assert}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "num_iterations = 10\n",
    "num_eval_episodes = 10\n",
    "eval_interval = int(num_iterations/100)\n",
    "log_interval = 1000\n",
    "\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent_ppo.train = common.function(agent_ppo.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent_ppo.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent_ppo.collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "returns\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent_ppo.train(experience).loss\n",
    "  # experience, unused_info = next(iterator)\n",
    "  # train_loss = agent_ppo.train(experience).loss\n",
    "\n",
    "\n",
    "  step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1:2f}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.025000000000002, 500.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, 1)\n",
    "# steps = range(0, num_iterations + 1, 10)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
