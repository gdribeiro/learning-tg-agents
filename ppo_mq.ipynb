{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in ./venvIntelliJ/lib/python3.9/site-packages (2.22.4)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (9.3.0)\r\n",
      "Requirement already satisfied: numpy in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (1.23.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./venvIntelliJ/lib/python3.9/site-packages (3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-agents in ./venvIntelliJ/lib/python3.9/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: absl-py>=0.6.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.3.0)\r\n",
      "Requirement already satisfied: gin-config>=0.4.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.5.0)\r\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.23.0)\r\n",
      "Requirement already satisfied: pygame==2.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.2.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.19.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Requirement already satisfied: protobuf>=3.11.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (3.19.6)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (4.4.0)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Requirement already satisfied: dm-tree in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.7)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyglet in ./venvIntelliJ/lib/python3.9/site-packages (2.0.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venvIntelliJ/lib/python3.9/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.0.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (22.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (9.3.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (4.38.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: numpy>=1.19 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venvIntelliJ/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./venvIntelliJ/lib/python3.9/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (14.0.6)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.28.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.23.5)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (4.4.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.19.6)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.51.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: setuptools in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (60.2.0)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.12.6)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvIntelliJ/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venvIntelliJ/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.13)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venvIntelliJ/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import array\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# PPO Imports\n",
    "##########################################################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import BoundedTensorSpec\n",
    "from tf_agents.specs import TensorSpec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.environments import py_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "PPOClipped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_EPSILON = 0.2\n",
    "GLOBAL_EPOCHS = 3\n",
    "\n",
    "# PPO Agent \n",
    "class PPOClipped:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Constants\n",
    "        self.actor_fc_layers = (64,64)\n",
    "        self.value_fc_layers = (64,64)\n",
    "        self.epsilon = GLOBAL_EPSILON\n",
    "        self.epochs = GLOBAL_EPOCHS\n",
    "\n",
    "        self.time_step_tensor_spec = tensor_spec.from_spec(env.time_step_spec())\n",
    "        self.observation_tensor_spec = tensor_spec.from_spec(env.observation_spec())\n",
    "        self.action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "\n",
    "        # Building the PPO Agent\n",
    "        self.actor_net = self.createActorNet()\n",
    "        self.value_net = self.createValueNet()\n",
    "        self.optimizer = self.createOptimizer()\n",
    "        self.train_step_counter = tf.Variable(0)\n",
    "\n",
    "        self.ppo_agent = self.createPPOAgent()\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.replay_buffer = self.createReplayBuffer()\n",
    "        self.iterator = self.createBufferIterator()\n",
    "\n",
    "\n",
    "    def createActorNet(self):\n",
    "        actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "            self.observation_tensor_spec,\n",
    "            self.action_tensor_spec,\n",
    "            fc_layer_params=self.actor_fc_layers,\n",
    "            activation_fn=tf.keras.activations.tanh,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "            seed_stream_class=tfp.util.SeedStream\n",
    "        )\n",
    "        return actor_net\n",
    "\n",
    "    def createValueNet(self):\n",
    "        value_net = value_network.ValueNetwork(\n",
    "            self.observation_tensor_spec,\n",
    "            fc_layer_params=self.value_fc_layers,\n",
    "            activation_fn=tf.keras.activations.tanh,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    "        )\n",
    "        return value_net\n",
    "\n",
    "    def createOptimizer(self):\n",
    "        initial_learning_rate = 1e-3\n",
    "        decay_steps = 1000\n",
    "        decay_rate = 0.96\n",
    "        learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    "        )\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "        learning_rate = 2.5e-4\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def createPPOAgent(self):\n",
    "        agent_ppo = ppo_clip_agent.PPOClipAgent(\n",
    "            time_step_spec=self.time_step_tensor_spec,\n",
    "            action_spec=self.action_tensor_spec,\n",
    "            optimizer=self.optimizer,\n",
    "            normalize_observations=True,\n",
    "            normalize_rewards=True,\n",
    "            actor_net=self.actor_net,\n",
    "            value_net=self.value_net,\n",
    "            importance_ratio_clipping=self.epsilon,\n",
    "            num_epochs=self.epochs,\n",
    "            use_gae=True,\n",
    "            train_step_counter=self.train_step_counter,\n",
    "        )\n",
    "        agent_ppo.initialize()\n",
    "        agent_ppo.train_step_counter.assign(0)\n",
    "        # (Optional) Optimize by wrapping some of this code in a graph using TF function.\n",
    "        agent_ppo.train = common.function(agent_ppo.train)\n",
    "        return agent_ppo\n",
    "\n",
    "    def createReplayBuffer(self):\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec= self.ppo_agent.collect_policy.trajectory_spec,\n",
    "            # self.ppo_agent.policy.trajectory_spec,\n",
    "            batch_size=1,\n",
    "            max_length=10000)\n",
    "        return replay_buffer\n",
    "\n",
    "    def addToBuffer(self, traj):\n",
    "        self.replay_buffer.add_batch(traj)\n",
    "        \n",
    "    def createBufferIterator(self):\n",
    "        n_step_update = 10\n",
    "        # batch_size = \n",
    "        batch_size = self.batch_size\n",
    "        dataset = self.replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "            num_steps=n_step_update + 1).prefetch(batch_size)\n",
    "        iterator = iter(dataset)\n",
    "        return iterator\n",
    "\n",
    "    def train(self):\n",
    "        experience, unused_info = next(self.iterator)\n",
    "        self.ppo_agent.train(experience)\n",
    "        print('Step Counter: {0}'.format(self.train_step_counter))\n",
    "\n",
    "    def getAction(self, time_step):\n",
    "        # return self.ppo_agent.policy.action(time_step)\n",
    "        return self.ppo_agent.collect_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MqEnvironment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "class MqEnvironment(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self, maxqos, minqos):\n",
    "        self._observation_spec = TensorSpec(shape=(8,), dtype=tf.float32, name='observation')\n",
    "        self._action_spec = BoundedTensorSpec(\n",
    "            shape=(), dtype=tf.int32, minimum=-1, maximum=1, name='action')\n",
    "        self._reward_spec = TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
    "\n",
    "        self._maxqos = maxqos\n",
    "        self._minqos = minqos\n",
    "\n",
    "        self._rewards = 0\n",
    "        self._current_time_step = None\n",
    "        self._action = None\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def reward_spec(self):\n",
    "        return self._reward_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        observation_zeros = tf.zeros((8,), dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(1, dtype=tf.float32)\n",
    "        self._current_time_step = ts.transition(observation_zeros, reward=reward, discount=1.0)\n",
    "        \n",
    "        return self._current_time_step\n",
    "    \n",
    "    def _step(self, action):\n",
    "        self._action = action\n",
    "        return self._current_time_step\n",
    "\n",
    "    def mq_step(self, action, state):\n",
    "        observation = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        reward = self.get_reward(observation)\n",
    "        self._current_time_step = ts.transition(observation, reward=reward, discount=1.0)\n",
    "        self._action = action\n",
    "        return self._current_time_step\n",
    "    \n",
    "    def get_reward(self, observation):\n",
    "        global_avg_spark, thpt_var, cDELAY, cTIMEP, RecSparkTotal, RecMQTotal, state, mem_use = observation.numpy()\n",
    "        lst_thpt_glo, lst_thpt_var, lst_cDELAY, lst_cTIMEP, lst_RecSparkTotal, lst_RecMQTotal, lst_state, lst_mem_use = self.current_time_step().observation.numpy()\n",
    "        r_thpt_glo, r_thpt_var, r_cDELAY, r_cTIMEP, r_RecSparkTotal, r_RecMQTotal, r_state, r_mem_use = np.zeros(8, dtype=np.float32)\n",
    "        \n",
    "        if lst_mem_use < self._minqos:\n",
    "            r_mem_use = -10\n",
    "        elif lst_mem_use > self._maxqos:\n",
    "            r_mem_use = -10\n",
    "        elif mem_use > lst_mem_use:\n",
    "            r_mem_use = 10\n",
    "        r_thpt_var = thpt_var\n",
    "        reward = r_thpt_glo + r_thpt_var + r_cDELAY + r_cTIMEP + r_RecSparkTotal + r_RecMQTotal + r_state + r_mem_use\n",
    "        \n",
    "        if reward < 0:\n",
    "            reward = 0.0\n",
    "        return tf.convert_to_tensor(reward, dtype=tf.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PPOAgentMQ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [],
   "source": [
    "class PPOAgentMQ:\n",
    "    def __init__(self, start_state, upper_limit, lower_limit):\n",
    "        # self.env = tf_py_environment.TFPyEnvironment(MqEnvironment(upper_limit, lower_limit))\n",
    "        self.env = MqEnvironment(upper_limit, lower_limit)\n",
    "        self.ppo_agent = PPOClipped(self.env)\n",
    "        self.buffer = False\n",
    "\n",
    "        # TimeStep of the environment\n",
    "        self._last_state = self.env.reset()\n",
    "        self._last_action = self.ppo_agent.getAction(self._last_state)\n",
    "        self._batch_size = self.ppo_agent.batch_size\n",
    "        # self.env._set_action(self._last_action)\n",
    "        self.env._current_action = self._last_action\n",
    "        self._last_reward = None\n",
    "        self._first_exec = True\n",
    "        \n",
    "    def step(self, _new_state):\n",
    "      \n",
    "        new_state = tf.convert_to_tensor(_new_state, dtype=tf.float32)\n",
    "        # Get the current state from the environment\n",
    "        last_time_step = self.env.current_time_step()\n",
    "        last_action = self._last_action\n",
    "        # From new state, calculate reward and change environment state\n",
    "        current_time_step = self.env.mq_step(last_action, new_state)\n",
    "        \n",
    "        traj = trajectory.from_transition(last_time_step, last_action, current_time_step)\n",
    "        traj_batched = tf.nest.map_structure(lambda t: tf.stack([t] * 1), traj)\n",
    "        self.ppo_agent.addToBuffer(traj_batched)\n",
    "        \n",
    "        if self.ppo_agent.replay_buffer.num_frames().numpy() > self._batch_size:\n",
    "            # for _ in range(64):\n",
    "            #     self.ppo_agent.addToBuffer(traj_batched)\n",
    "            # self._first_exec = False\n",
    "            # \n",
    "            self.ppo_agent.train()\n",
    "        \n",
    "        # -----------------------------\n",
    "\n",
    "        # Collect initial data\n",
    "        # initial_time_step = train_env.reset()\n",
    "        # while not initial_time_step.is_last():\n",
    "        #     action_step = agent.collect_policy.action(initial_time_step)\n",
    "        #     next_time_step = train_env.step(action_step.action)\n",
    "        #     traj = trajectory.from_transition(initial_time_step, action_step, next_time_step)\n",
    "        #     replay_buffer.add_batch(traj)\n",
    "        #     initial_time_step = next_time_step\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def finish(self, last_state):\n",
    "        pass\n",
    "        # with open(CSVFILENAME, mode='a+') as episode_file:\n",
    "        #     writer = csv.writer(episode_file, delimiter=',')\n",
    "        # \n",
    "        #     writer.writerow([self.episode, self.epsilon, mean_squared_reward])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RUN Env Simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n",
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=6>\n",
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=9>\n",
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=12>\n",
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=15>\n",
      "Step Counter: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=18>\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "sim_iterations = 70\n",
    "# ---------------\n",
    "\n",
    "maxqos = 1000\n",
    "minqos = 10\n",
    "\n",
    "start_state = np.zeros((8,)).tolist()\n",
    "\n",
    "agent = PPOAgentMQ(start_state, maxqos, minqos)\n",
    "\n",
    "for _ in range(sim_iterations):\n",
    "    state = np.random.rand(8).astype(np.dtype('float32')).tolist()\n",
    "    action = agent.step(state)\n",
    "\n",
    "\n",
    "last_state = [0,0,0,0,0,0,0,0]\n",
    "agent.finish(last_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    i =next(agent.ppo_agent.iterator)\n",
    "    print(i)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_state = np.zeros((8,)).tolist()\n",
    "\n",
    "env = MqEnvironment(minqos, maxqos)\n",
    "# env = tf_py_environment.TFPyEnvironment(MqEnvironment(maxqos, minqos))\n",
    "pa = PPOClipped(env)\n",
    "mq = PPOAgentMQ(start_state, maxqos, minqos)\n",
    "\n",
    "# pa.getAction(env.reset()).action.numpy()\n",
    "# pa.getAction(env.reset())\n",
    "print(pa.ppo_agent.collect_policy.policy_step_spec)\n",
    "print(pa.ppo_agent.policy.policy_step_spec)\n",
    "\n",
    "# \n",
    "# print(env.action_spec())\n",
    "# print(pa.action_tensor_spec)\n",
    "# \n",
    "# print(env.reset())\n",
    "# print(pa.getAction(env.reset()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = np.zeros((8,)).tolist()\n",
    "qosmax = 1000\n",
    "qosmin = 10\n",
    "# ppomq = PPOAgentMQ(state, qosmax, qosmin)\n",
    "# \n",
    "# ppomq.ppo_agent.ppo_agent.collect_policy.time_step_spec \n",
    "# mts=tf.convert_to_tensor([0,0,0,0,0,0,0,0], dtype=np.dtype('float32'))\n",
    "# time_step = ts.transition(mts, reward=tf.convert_to_tensor(1, dtype=np.dtype('int32')), discount=1.0)\n",
    "# action = ppomq.ppo_agent.getAction(time_step)\n",
    "# \n",
    "# ppomq.env._current_timestep = time_step\n",
    "# ppomq.env.step(action, time_step)\n",
    "\n",
    "# ppomq.ppo_agent.ppo_agent.policy.action(ts.transition(mts, reward=tf.convert_to_tensor(1, dtype=np.dtype('int32')), discount=1.0))\n",
    "# tf.convert_to_tensor([0,0,0,0,0,0,0,0]).numpy().tolist()\n",
    "# tf.convert_to_tensor([0,0,0,0,0,0,0,0], dtype=np.dtype('float32'))\n",
    "\n",
    "# mqenv = tf_py_environment.TFPyEnvironment(MqEnvironment(10,1000))\n",
    "# mqenv.reset()\n",
    "# ts = mqenv.reset()\n",
    "\n",
    "# mqenv.current_time_step()[3].numpy()[0].tolist()\n",
    "# tf.convert_to_tensor(state, dtype=np.dtype('float32'))\n",
    "\n",
    "\n",
    "env = MqEnvironment(qosmax, qosmin)\n",
    "env.action_spec()\n",
    "# ppomq = PPOAgentMQ(state, qosmax, qosmin)\n",
    "time_step_tensor_spec = env.time_step_spec()\n",
    "observation_tensor_spec = env.observation_spec()\n",
    "action_tensor_spec = env.action_spec()\n",
    "\n",
    "time_step_tensor_spec\n",
    "observation_tensor_spec\n",
    "action_tensor_spec\n",
    "\n",
    "# actor_distribution_network.ActorDistributionNetwork(\n",
    "#     observation_tensor_spec,\n",
    "#     action_tensor_spec,\n",
    "#     fc_layer_params=(64,64),\n",
    "#     activation_fn=tf.keras.activations.tanh,\n",
    "#     kernel_initializer=tf.keras.initializers.Orthogonal(),\n",
    "#     seed_stream_class=tfp.util.SeedStream\n",
    "# )\n",
    "\n",
    "print(tensor_spec.from_spec(observation_tensor_spec))\n",
    "print(tf.convert_to_tensor(state))\n",
    "# print(tf.convert_to_tensor(state, dtype=tf.int32))\n",
    "\n",
    "# r = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "# r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# print(tensor_spec.from_spec(observation_tensor_spec))\n",
    "# print(tf.convert_to_tensor(state))\n",
    "# print(tf.convert_to_tensor(state, dtype=tf.int32))\n",
    "\n",
    "# r = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "\n",
    "# obs_tensor_spec = tensor_spec.from_spec(observation_tensor_spec)\n",
    "# obs_tensor = tf.convert_to_tensor(np.zeros((8,)).tolist())\n",
    "\n",
    "# obs_tensor_spec\n",
    "# obs_tensor\n",
    "# state = np.zeros((8,)).tolist()\n",
    "# reward = 100\n",
    "# current_time_step = ts.transition(state, reward=reward, discount=1.0)\n",
    "# print(current_time_step)\n",
    "# current_time_step = ts.transition(current_time_step.observation, reward=100, discount=1.0)\n",
    "# current_time_step"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
