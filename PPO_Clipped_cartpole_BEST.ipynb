{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in ./venvIntelliJ/lib/python3.9/site-packages (2.22.4)\r\n",
      "Requirement already satisfied: numpy in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (1.23.5)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (9.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./venvIntelliJ/lib/python3.9/site-packages (3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-agents in ./venvIntelliJ/lib/python3.9/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Requirement already satisfied: pygame==2.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.1.0)\r\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.23.0)\r\n",
      "Requirement already satisfied: absl-py>=0.6.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.3.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.19.0)\r\n",
      "Requirement already satisfied: gin-config>=0.4.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.5.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Requirement already satisfied: protobuf>=3.11.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (3.19.6)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (4.4.0)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\r\n",
      "Requirement already satisfied: dm-tree in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyglet in ./venvIntelliJ/lib/python3.9/site-packages (2.0.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venvIntelliJ/lib/python3.9/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (22.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (4.38.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.0.6)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.19 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from matplotlib) (9.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venvIntelliJ/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./venvIntelliJ/lib/python3.9/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.12.6)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.51.1)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (4.4.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.19.6)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (14.0.6)\r\n",
      "Requirement already satisfied: numpy>=1.20 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (1.23.5)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (60.2.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (0.28.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: packaging in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (22.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvIntelliJ/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venvIntelliJ/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venvIntelliJ/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.13)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venvIntelliJ/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvIntelliJ/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venvIntelliJ/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venvIntelliJ/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 21:49:34.020715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Seed for PPO actor network\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# PPO Agent\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "from tf_agents.agents.ppo import ppo_actor_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "# old agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "env_name = \"Acrobot-v1\"\n",
    "batch_size = 64  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_tensor_spec = tensor_spec.from_spec(train_env.observation_spec())\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "time_step_tensor_spec = tensor_spec.from_spec(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='observation', minimum=array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
      "       -28.274334], dtype=float32), maximum=array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n",
      "      dtype=float32))\n",
      "Action: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2))\n",
      "TimeStep: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='observation', minimum=array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
      "       -28.274334], dtype=float32), maximum=array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('Observation: {0}'.format(observation_tensor_spec))\n",
    "print('Action: {0}'.format(action_tensor_spec))\n",
    "print('TimeStep: {0}'.format(time_step_tensor_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netowork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_net_builder = ppo_actor_network.PPOActorNetwork()\n",
    "# actor_net = actor_net_builder.create_sequential_actor_net(\n",
    "#     actor_fc_layers, action_tensor_spec)\n",
    "\n",
    "\n",
    "actor_fc_layers = (64,64)\n",
    "value_fc_layers = (64,64)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_tensor_spec,\n",
    "        action_tensor_spec,\n",
    "        fc_layer_params=actor_fc_layers,\n",
    "        activation_fn=tf.nn.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.Orthogonal(seed=1),\n",
    "        seed_stream_class=tfp.util.SeedStream(seed=None, salt='tf_agents_sequential_layers')\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='observation', minimum=array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
      "       -28.274334], dtype=float32), maximum=array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n",
      "      dtype=float32))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2))\n",
      "TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='observation', minimum=array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
      "       -28.274334], dtype=float32), maximum=array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print(observation_tensor_spec)\n",
    "print(action_tensor_spec)\n",
    "print(time_step_tensor_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 21:49:38.274238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.2\n",
    "epsilon = 0.2\n",
    "discount = 0.99\n",
    "lambda_value = 0.95\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "\n",
    "# learning_rate = 3e-4\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "initial_learning_rate = 10e-3\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.96\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False) Uset the same trin_step_counter\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "agent_ppo = ppo_clip_agent.PPOClipAgent(\n",
    "    time_step_spec=time_step_tensor_spec,\n",
    "    action_spec=action_tensor_spec,\n",
    "    optimizer=optimizer,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    greedy_eval=False,\n",
    "    lambda_value=lambda_value,\n",
    "    discount_factor=discount,\n",
    "    importance_ratio_clipping=epsilon,\n",
    "    num_epochs=25,\n",
    "    normalize_observations=True,\n",
    "    normalize_rewards=True,\n",
    "    use_gae=True,\n",
    "    # log_prob_clipping=epsilon,\n",
    "    # gradient_clipping=epsilon,\n",
    "    # value_clipping=epsilon,\n",
    "    train_step_counter=train_step_counter,\n",
    ")\n",
    "agent_ppo.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(\n{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\narray([[ 0.99998146, -0.00609106,  0.9990883 ,  0.04269192, -0.00122559,\n         0.0051249 ]], dtype=float32)>,\n 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500.0\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    info_spec=time_step_tensor_spec)\n",
    "\n",
    "num_eval_episodes=1\n",
    "def evaluate_policy(policy):\n",
    "  for _ in range(1):\n",
    "    avg= compute_avg_return(eval_env, policy, num_eval_episodes)\n",
    "    print(avg)\n",
    "\n",
    "# evaluate_policy(random_policy)\n",
    "\n",
    "# evaluate_policy(agent_ppo.policy)\n",
    "\n",
    "evaluate_policy(agent_ppo.collect_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "_TupleWrapper(Trajectory(\n{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2)),\n 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n 'observation': BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='observation', minimum=array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n       -28.274334], dtype=float32), maximum=array([ 1.      ,  1.      ,  1.      ,  1.      , 12.566371, 28.274334],\n      dtype=float32)),\n 'policy_info': {'dist_params': {'logits': TensorSpec(shape=(3,), dtype=tf.float32, name='CategoricalProjectionNetwork_logits')}},\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_ppo.collect_policy.trajectory_spec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99998146, -0.00609106,  0.9990883 ,  0.04269192, -0.00122559,\n",
      "         0.0051249 ]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.35860622, 0.00586434, 0.06976362]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9998371 , -0.01804963,  0.99733675,  0.07293405, -0.11480132,\n",
      "         0.29035622]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99998146, -0.00609106,  0.9990883 ,  0.04269192, -0.00122559,\n",
      "         0.0051249 ]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.35860622, 0.00586434, 0.06976362]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9998371 , -0.01804963,  0.99733675,  0.07293405, -0.11480132,\n",
      "         0.29035622]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.28111768,  0.03106195, -0.12023945]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997373 , -0.02291788,  0.99639815,  0.08479822,  0.06715835,\n",
      "        -0.17354804]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9998371 , -0.01804963,  0.99733675,  0.07293405, -0.11480132,\n",
      "         0.29035622]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.28111768,  0.03106195, -0.12023945]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997373 , -0.02291788,  0.99639815,  0.08479822,  0.06715835,\n",
      "        -0.17354804]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.24772258, -0.05914902,  0.11859495]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99981624, -0.01916956,  0.99717087,  0.07516842, -0.03058145,\n",
      "         0.0789028 ]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997373 , -0.02291788,  0.99639815,  0.08479822,  0.06715835,\n",
      "        -0.17354804]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.24772258, -0.05914902,  0.11859495]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99981624, -0.01916956,  0.99717087,  0.07516842, -0.03058145,\n",
      "         0.0789028 ]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.32277283, 0.01136243, 0.01922887]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99940616, -0.03445834,  0.9933942 ,  0.11475172, -0.11784598,\n",
      "         0.30919284]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99981624, -0.01916956,  0.99717087,  0.07516842, -0.03058145,\n",
      "         0.0789028 ]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.32277283, 0.01136243, 0.01922887]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99940616, -0.03445834,  0.9933942 ,  0.11475172, -0.11784598,\n",
      "         0.30919284]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.25648138,  0.02188621, -0.12932618]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9979806 , -0.06351924,  0.9814191 ,  0.19187649, -0.16463919,\n",
      "         0.45328492]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99940616, -0.03445834,  0.9933942 ,  0.11475172, -0.11784598,\n",
      "         0.30919284]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.25648138,  0.02188621, -0.12932618]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9979806 , -0.06351924,  0.9814191 ,  0.19187649, -0.16463919,\n",
      "         0.45328492]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.2160478 ,  0.03775444, -0.16256833]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9975327 , -0.07020342,  0.9758792 ,  0.21831127,  0.09921494,\n",
      "        -0.18764235]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9979806 , -0.06351924,  0.9814191 ,  0.19187649, -0.16463919,\n",
      "         0.45328492]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.2160478 ,  0.03775444, -0.16256833]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9975327 , -0.07020342,  0.9758792 ,  0.21831127,  0.09921494,\n",
      "        -0.18764235]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.14644541, -0.05119878,  0.20109737]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99864095, -0.05211738,  0.98207325,  0.18849954,  0.07742036,\n",
      "        -0.10894776]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9975327 , -0.07020342,  0.9758792 ,  0.21831127,  0.09921494,\n",
      "        -0.18764235]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.14644541, -0.05119878,  0.20109737]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99864095, -0.05211738,  0.98207325,  0.18849954,  0.07742036,\n",
      "        -0.10894776]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.17949452, -0.06367328,  0.16789979]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9991655 , -0.04084551,  0.9840183 ,  0.17806728,  0.0328768 ,\n",
      "         0.00625459]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.99864095, -0.05211738,  0.98207325,  0.18849954,  0.07742036,\n",
      "        -0.10894776]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.17949452, -0.06367328,  0.16789979]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9991655 , -0.04084551,  0.9840183 ,  0.17806728,  0.0328768 ,\n",
      "         0.00625459]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.22527373, -0.05592058,  0.09605927]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9992189 , -0.03951756,  0.9815311 ,  0.19130267, -0.01935341,\n",
      "         0.12615943]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9991655 , -0.04084551,  0.9840183 ,  0.17806728,  0.0328768 ,\n",
      "         0.00625459]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.22527373, -0.05592058,  0.09605927]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9992189 , -0.03951756,  0.9815311 ,  0.19130267, -0.01935341,\n",
      "         0.12615943]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.23801968, -0.02515579, -0.00695118]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997651 , -0.02167282,  0.98731726,  0.15875952,  0.19313438,\n",
      "        -0.44814312]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9992189 , -0.03951756,  0.9815311 ,  0.19130267, -0.01935341,\n",
      "         0.12615943]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.23801968, -0.02515579, -0.00695118]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "WARNING:tensorflow:From /Users/ribeirg/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    agent_ppo.collect_policy.trajectory_spec,\n",
    "    batch_size=1,\n",
    "    max_length=10000)\n",
    "    # max_length=10000)\n",
    "\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    print(time_step)\n",
    "    action_step = policy.action(time_step)\n",
    "    print(action_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    print(next_time_step)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    print(traj)\n",
    "    # print(next_time_step.reward)\n",
    "\n",
    "initial_collect_steps=10\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "n_step_update=1\n",
    "initial_collect_steps=10\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(batch_size)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# print(f'TimeStep: {train_env.time_step_spec()}')\n",
    "# print(f'Action: {train_env.action_spec()}')\n",
    "# print(f'Agent: {agent_ppo.collect_policy.time_step_spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997651 , -0.02167282,  0.98731726,  0.15875952,  0.19313438,\n",
      "        -0.44814312]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info={'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.11474635, -0.10717109,  0.15675601]], dtype=float32)>}})\n",
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9994349 ,  0.03361329,  0.9997695 ,  0.02146881,  0.34426832,\n",
      "        -0.89915526]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n",
      "Trajectory(\n",
      "{'action': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9997651 , -0.02167282,  0.98731726,  0.15875952,  0.19313438,\n",
      "        -0.44814312]], dtype=float32)>,\n",
      " 'policy_info': {'dist_params': {'logits': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.11474635, -0.10717109,  0.15675601]], dtype=float32)>}},\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 48\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Sample a batch of data from the buffer and update the agent's network.\u001B[39;00m\n\u001B[1;32m     47\u001B[0m experience, unused_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m---> 48\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43magent_ppo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# experience, unused_info = next(iterator)\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# train_loss = agent_ppo.train(experience).loss\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (train_step_counter\u001B[38;5;241m.\u001B[39mvalue() \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1020\u001B[39m):\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:945\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall through to cond-based initialization.\u001B[39;00m\n\u001B[1;32m    942\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    943\u001B[0m     \u001B[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001B[39;00m\n\u001B[1;32m    944\u001B[0m     \u001B[38;5;66;03m# no_variable_creation function.\u001B[39;00m\n\u001B[0;32m--> 945\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    947\u001B[0m   _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    948\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn\u001B[38;5;241m.\u001B[39m_function_spec  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    949\u001B[0m       \u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(\n\u001B[1;32m    950\u001B[0m           args, kwds))\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:133\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[0;32m--> 133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\u001B[38;5;241m.\u001B[39m_call_flat(\n\u001B[1;32m    135\u001B[0m     filtered_flat_args, captured_inputs\u001B[38;5;241m=\u001B[39mconcrete_function\u001B[38;5;241m.\u001B[39mcaptured_inputs)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001B[0m, in \u001B[0;36mTracingCompiler._maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m    357\u001B[0m   \u001B[38;5;66;03m# Only get placeholders for arguments, not captures\u001B[39;00m\n\u001B[1;32m    358\u001B[0m   args, kwargs \u001B[38;5;241m=\u001B[39m generalized_func_key\u001B[38;5;241m.\u001B[39m_placeholder_value()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m--> 360\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_concrete_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    362\u001B[0m graph_capture_container \u001B[38;5;241m=\u001B[39m concrete_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39m_capture_func_lib  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;66;03m# Maintain the list of all captures\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001B[0m, in \u001B[0;36mTracingCompiler._create_concrete_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m missing_arg_names \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (arg, i) \u001B[38;5;28;01mfor\u001B[39;00m i, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(missing_arg_names)\n\u001B[1;32m    281\u001B[0m ]\n\u001B[1;32m    282\u001B[0m arg_names \u001B[38;5;241m=\u001B[39m base_arg_names \u001B[38;5;241m+\u001B[39m missing_arg_names\n\u001B[1;32m    283\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m monomorphic_function\u001B[38;5;241m.\u001B[39mConcreteFunction(\n\u001B[0;32m--> 284\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[1;32m    295\u001B[0m     spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1325\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001B[0m\n\u001B[1;32m   1319\u001B[0m   \u001B[38;5;66;03m# Returning a closed-over tensor does not trigger convert_to_tensor.\u001B[39;00m\n\u001B[1;32m   1320\u001B[0m   func_graph\u001B[38;5;241m.\u001B[39moutputs\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m   1321\u001B[0m       func_graph\u001B[38;5;241m.\u001B[39mcapture(x)\n\u001B[1;32m   1322\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m flatten(func_graph\u001B[38;5;241m.\u001B[39mstructured_outputs)\n\u001B[1;32m   1323\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1325\u001B[0m   func_graph\u001B[38;5;241m.\u001B[39mvariables \u001B[38;5;241m=\u001B[39m variables\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m add_control_dependencies:\n\u001B[1;32m   1328\u001B[0m   func_graph\u001B[38;5;241m.\u001B[39mcontrol_outputs\u001B[38;5;241m.\u001B[39mextend(deps_control_manager\u001B[38;5;241m.\u001B[39mops_which_must_run)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/framework/auto_control_deps.py:547\u001B[0m, in \u001B[0;36mAutomaticControlDependencies.__exit__\u001B[0;34m(self, unused_type, unused_value, unused_traceback)\u001B[0m\n\u001B[1;32m    541\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m control_inputs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_building_function:\n\u001B[1;32m    542\u001B[0m     control_inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    543\u001B[0m         c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m control_inputs\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m c\u001B[38;5;241m.\u001B[39m_control_flow_context \u001B[38;5;129;01mis\u001B[39;00m op\u001B[38;5;241m.\u001B[39m_control_flow_context\n\u001B[1;32m    545\u001B[0m     ]\n\u001B[0;32m--> 547\u001B[0m   \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_control_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontrol_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;66;03m# Record the ops which first use resources touched by \"ops which must run\".\u001B[39;00m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecord_initial_resource_uses:\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2450\u001B[0m, in \u001B[0;36mOperation._add_control_inputs\u001B[0;34m(self, ops)\u001B[0m\n\u001B[1;32m   2440\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_add_control_inputs\u001B[39m(\u001B[38;5;28mself\u001B[39m, ops):\n\u001B[1;32m   2441\u001B[0m   \u001B[38;5;124;03m\"\"\"Add a list of new control inputs to this operation.\u001B[39;00m\n\u001B[1;32m   2442\u001B[0m \n\u001B[1;32m   2443\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2448\u001B[0m \u001B[38;5;124;03m    ValueError: if any op in ops is from a different graph.\u001B[39;00m\n\u001B[1;32m   2449\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2450\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39m_c_graph\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;28;01mas\u001B[39;00m c_graph:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   2451\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m op \u001B[38;5;129;01min\u001B[39;00m ops:\n\u001B[1;32m   2452\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(op, Operation):\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.13_3/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:119\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/framework/c_api_util.py:68\u001B[0m, in \u001B[0;36mUniquePtr.get\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m AlreadyGarbageCollectedError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_name)\n\u001B[0;32m---> 68\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_obj\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "num_iterations = 10000\n",
    "collect_steps_per_iteration = 1\n",
    "eval_interval = 10\n",
    "num_eval_episodes = 1\n",
    "\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent_ppo.train = common.function(agent_ppo.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent_ppo.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent_ppo.collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "returns\n",
    "# \n",
    "# for _ in range(num_iterations):\n",
    "#   # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "#   for _ in range(collect_steps_per_iteration):\n",
    "#     collect_step(train_env, agent_ppo.collect_policy)\n",
    "#   # Sample a batch of data from the buffer and update the agent's network.\n",
    "#   experience, unused_info = next(iterator)\n",
    "#   train_loss = agent_ppo.train(experience).loss\n",
    "#   # experience, unused_info = next(iterator)\n",
    "#   # train_loss = agent_ppo.train(experience).loss\n",
    "# \n",
    "#   step = agent_ppo.train_step_counter.numpy()\n",
    "# \n",
    "#   if step % eval_interval == 0:\n",
    "#     avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "#     print('step = {0}: loss = {1}  AVG RETURN: {2:2f}'.format(step, train_loss, avg_return))\n",
    "# \n",
    "#   returns.append(avg_return)\n",
    "\n",
    "new_epsilon = epsilon\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent_ppo.collect_policy)\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent_ppo.train(experience).loss\n",
    "    # experience, unused_info = next(iterator)\n",
    "    # train_loss = agent_ppo.train(experience).loss\n",
    "    if not (train_step_counter.value() % 1020):\n",
    "        new_epsilon = new_epsilon - 0.01\n",
    "        if new_epsilon < 0.01:\n",
    "            new_epsilon = 0.01\n",
    "        agent_ppo._importance_ratio_clipping = new_epsilon\n",
    "        print('**** new_epsilon = {0} ***'.format(new_epsilon))\n",
    "\n",
    "\n",
    "    step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "        print('step = {0}: loss = {1}  AVG RETURN: {2:2f}  Epsilon: {3}'.format(step, train_loss, avg_return, agent_ppo._importance_ratio_clipping))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "5001"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(returns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10001,) and (5001,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[98], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, num_iterations \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# steps = range(0, num_iterations + 1, 10)\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAverage Return\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStep\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/matplotlib/pyplot.py:2740\u001B[0m, in \u001B[0;36mplot\u001B[0;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2738\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[1;32m   2739\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\u001B[38;5;241m*\u001B[39margs, scalex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, scaley\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 2740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2741\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2742\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1662\u001B[0m, in \u001B[0;36mAxes.plot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1420\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[1;32m   1421\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1659\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[1;32m   1660\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1661\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[0;32m-> 1662\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[1;32m   1663\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m   1664\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[0;34m(self, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    310\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes\u001B[38;5;241m.\u001B[39myaxis\u001B[38;5;241m.\u001B[39mupdate_units(y)\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 504\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y must have same first dimension, but \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    505\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave shapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y can be no greater than 2D, but have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    508\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (10001,) and (5001,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, 1)\n",
    "# steps = range(0, num_iterations + 1, 10)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=max(returns) + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
