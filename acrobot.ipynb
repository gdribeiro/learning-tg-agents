{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in ./venvIntelliJ/lib/python3.9/site-packages (2.22.4)\r\n",
      "Requirement already satisfied: numpy in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (1.23.5)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from imageio) (9.3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./venvIntelliJ/lib/python3.9/site-packages (3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-agents in ./venvIntelliJ/lib/python3.9/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (4.4.0)\r\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.19.0)\r\n",
      "Requirement already satisfied: protobuf>=3.11.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (3.19.6)\r\n",
      "Requirement already satisfied: pillow in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (9.3.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.14.1)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.16.0)\r\n",
      "Requirement already satisfied: absl-py>=0.6.1 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.3.0)\r\n",
      "Requirement already satisfied: gin-config>=0.4.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.5.0)\r\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (0.23.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.2.0)\r\n",
      "Requirement already satisfied: pygame==2.1.0 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venvIntelliJ/lib/python3.9/site-packages (from tf-agents) (1.23.5)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in ./venvIntelliJ/lib/python3.9/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (5.1.0)\r\n",
      "Requirement already satisfied: dm-tree in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.7)\r\n",
      "Requirement already satisfied: decorator in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (5.1.1)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in ./venvIntelliJ/lib/python3.9/site-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venvIntelliJ/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.11.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyglet in ./venvIntelliJ/lib/python3.9/site-packages (2.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tf-agents\n",
    "%pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Seed for PPO actor network\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import tf_agents as ta\n",
    "\n",
    "# PPO Agent\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.agents.ppo import ppo_actor_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "\n",
    "from tf_agents.drivers import py_driver\n",
    "\n",
    "# old agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import categorical_q_network\n",
    "from tf_agents.networks import lstm_encoding_network\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "VersionNotFound",
     "evalue": "Environment version `v2` for `Acrobot` could not be found. `Acrobot` provides the versioned environments: [ `v1` ].\n  In call to configurable 'load' (<function load at 0x131957310>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mVersionNotFound\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[235], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m env_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcrobot-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# @param {type:\"string\"}\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m train_py_env \u001B[38;5;241m=\u001B[39m \u001B[43msuite_gym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m eval_py_env \u001B[38;5;241m=\u001B[39m suite_gym\u001B[38;5;241m.\u001B[39mload(env_name)\n\u001B[1;32m      6\u001B[0m train_env \u001B[38;5;241m=\u001B[39m tf_py_environment\u001B[38;5;241m.\u001B[39mTFPyEnvironment(train_py_env)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gin/config.py:1605\u001B[0m, in \u001B[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1603\u001B[0m scope_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in scope \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(scope_str) \u001B[38;5;28;01mif\u001B[39;00m scope_str \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1604\u001B[0m err_str \u001B[38;5;241m=\u001B[39m err_str\u001B[38;5;241m.\u001B[39mformat(name, fn_or_cls, scope_info)\n\u001B[0;32m-> 1605\u001B[0m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_exception_message_and_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merr_str\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gin/utils.py:41\u001B[0m, in \u001B[0;36maugment_exception_message_and_reraise\u001B[0;34m(exception, message)\u001B[0m\n\u001B[1;32m     39\u001B[0m proxy \u001B[38;5;241m=\u001B[39m ExceptionProxy()\n\u001B[1;32m     40\u001B[0m ExceptionProxy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(exception)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m proxy\u001B[38;5;241m.\u001B[39mwith_traceback(exception\u001B[38;5;241m.\u001B[39m__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gin/config.py:1582\u001B[0m, in \u001B[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1579\u001B[0m new_kwargs\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[1;32m   1581\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1582\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnew_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnew_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1583\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m   1584\u001B[0m   err_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/environments/suite_gym.py:83\u001B[0m, in \u001B[0;36mload\u001B[0;34m(environment_name, discount, max_episode_steps, gym_env_wrappers, env_wrappers, spec_dtype_map, gym_kwargs, render_kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124;03m\"\"\"Loads the selected environment and wraps it with the specified wrappers.\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03mNote that by default a TimeLimit wrapper is used to limit episode lengths\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m  A PyEnvironment instance.\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     82\u001B[0m gym_kwargs \u001B[38;5;241m=\u001B[39m gym_kwargs \u001B[38;5;28;01mif\u001B[39;00m gym_kwargs \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m---> 83\u001B[0m gym_spec \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec\u001B[49m\u001B[43m(\u001B[49m\u001B[43menvironment_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m gym_env \u001B[38;5;241m=\u001B[39m gym_spec\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgym_kwargs)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_episode_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gym_spec\u001B[38;5;241m.\u001B[39mmax_episode_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gym/envs/registration.py:680\u001B[0m, in \u001B[0;36mspec\u001B[0;34m(id)\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mspec\u001B[39m(\u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EnvSpec:\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mregistry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gym/envs/registration.py:540\u001B[0m, in \u001B[0;36mEnvRegistry.spec\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;28mid\u001B[39m \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m    538\u001B[0m \u001B[38;5;66;03m# We can go ahead and return the env_spec.\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m# The EnvSpecTree will take care of any exceptions.\u001B[39;00m\n\u001B[0;32m--> 540\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv_specs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gym/envs/registration.py:376\u001B[0m, in \u001B[0;36mEnvSpecTree.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EnvSpec:\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;66;03m# Get an item from the tree.\u001B[39;00m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;66;03m# We first parse the components so we can look up the\u001B[39;00m\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;66;03m# appropriate environment ID.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m     namespace, name, version \u001B[38;5;241m=\u001B[39m parse_env_id(key)\n\u001B[0;32m--> 376\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assert_version_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree[namespace][name][version]\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/gym/envs/registration.py:369\u001B[0m, in \u001B[0;36mEnvSpecTree._assert_version_exists\u001B[0;34m(self, namespace, name, version)\u001B[0m\n\u001B[1;32m    367\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m ]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    368\u001B[0m message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 369\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mVersionNotFound(message)\n",
      "\u001B[0;31mVersionNotFound\u001B[0m: Environment version `v2` for `Acrobot` could not be found. `Acrobot` provides the versioned environments: [ `v1` ].\n  In call to configurable 'load' (<function load at 0x131957310>)"
     ]
    }
   ],
   "source": [
    "env_name = \"Acrobot-v2\"  # @param {type:\"string\"}\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_tensor_spec = tensor_spec.from_spec(train_env.observation_spec())\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "time_step_tensor_spec = tensor_spec.from_spec(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observation: {0}'.format(observation_tensor_spec))\n",
    "print('Action: {0}'.format(action_tensor_spec))\n",
    "print('TimeStep: {0}'.format(time_step_tensor_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor_fc_layers =(64,64)\n",
    "value_fc_layers = (64,64)\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    fc_layer_params=actor_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    seed_stream_class=tfp.util.SeedStream(seed=None, salt='tf_agents_sequential_layers')\n",
    ")\n",
    "\n",
    "value_net = value_network.ValueNetwork(\n",
    "    observation_tensor_spec,\n",
    "    fc_layer_params=value_fc_layers,\n",
    "    activation_fn=tf.keras.activations.tanh,\n",
    "    kernel_initializer=tf.keras.initializers.Orthogonal()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  4608      \n",
      "_________________________________________________________________\n",
      "CategoricalProjectionNetwork multiple                  195       \n",
      "=================================================================\n",
      "Total params: 4,803\n",
      "Trainable params: 4,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"ValueNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  4608      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,673\n",
      "Trainable params: 4,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "epsilon = 0.2\n",
    "discount = 0.99\n",
    "gae = 0.95\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent_ppo = ppo_clip_agent.PPOClipAgent(\n",
    "    time_step_spec=time_step_tensor_spec,\n",
    "    action_spec=action_tensor_spec,\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    optimizer=optimizer,\n",
    "    use_gae=True,\n",
    "    importance_ratio_clipping=epsilon,\n",
    "    # log_prob_clipping=epsilon,\n",
    "    # gradient_clipping=epsilon,\n",
    "    # value_clipping=epsilon,\n",
    "    train_step_counter=train_step_counter,\n",
    "    # compute_value_and_advantage_in_train=True,\n",
    "    # update_normalizers_in_train=False,\n",
    "    # num_epochs=10\n",
    ")\n",
    "agent_ppo.initialize()\n",
    "\n",
    "agent_ppo.actor_net.summary()\n",
    "\n",
    "agent_ppo._value_net.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        type(time_step)\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step, policy.get_initial_state(batch_size=1))\n",
    "            time_step = environment.step(action_step.action)\n",
    "            # print('time step: {0}'.format(time_step))\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_tensor_spec,\n",
    "    action_tensor_spec,\n",
    "    info_spec=time_step_tensor_spec)\n",
    "\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "    for _ in range(1):\n",
    "        avg = compute_avg_return(eval_env, policy, num_eval_episodes)\n",
    "        print(avg)\n",
    "\n",
    "# evaluate_policy(random_policy)\n",
    "# \n",
    "# evaluate_policy(agent_ppo.policy)\n",
    "# \n",
    "# evaluate_policy(agent_ppo.collect_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "replay_buffer_size = 10000\n",
    "replay_buffer_batch_size = 1\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent_ppo.collect_policy.trajectory_spec,\n",
    "    batch_size=replay_buffer_batch_size,\n",
    "    max_length=replay_buffer_size\n",
    ")\n",
    "rb_observer = replay_buffer.add_batch\n",
    "\n",
    "\n",
    "# agent_ppo.collect_policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    policy_state = policy._get_initial_state(environment.batch_size)\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step, policy_state)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    # print('Log Trajectory: {0}'.format(traj))\n",
    "    replay_buffer.add_batch(traj)\n",
    "    # print(next_time_step.reward)\n",
    "    \n",
    "collect_step(train_env, agent_ppo.collect_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_steps = 10\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, agent_ppo.collect_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step_update = 1\n",
    "# batch_size = 64\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 50: loss: 21.132577896118164 Average Return = -500.000000\n",
      "step = 100: loss: 21.123008728027344 Average Return = -500.000000\n",
      "step = 150: loss: 20.894039154052734 Average Return = -500.000000\n",
      "step = 200: loss: 21.188030242919922 Average Return = -500.000000\n",
      "step = 250: loss: 21.15846061706543 Average Return = -500.000000\n",
      "step = 300: loss: 21.14564323425293 Average Return = -500.000000\n",
      "step = 350: loss: 21.222286224365234 Average Return = -500.000000\n",
      "step = 400: loss: 21.28995132446289 Average Return = -500.000000\n",
      "step = 450: loss: 21.16971778869629 Average Return = -500.000000\n",
      "step = 500: loss: 25.47967529296875 Average Return = -500.000000\n",
      "step = 550: loss: 20.967836380004883 Average Return = -500.000000\n",
      "step = 600: loss: 21.14879608154297 Average Return = -500.000000\n",
      "step = 650: loss: 20.96845245361328 Average Return = -500.000000\n",
      "step = 700: loss: 20.86527442932129 Average Return = -500.000000\n",
      "step = 750: loss: 20.84653663635254 Average Return = -500.000000\n",
      "step = 800: loss: 20.74696922302246 Average Return = -500.000000\n",
      "step = 850: loss: 20.761646270751953 Average Return = -500.000000\n",
      "step = 900: loss: 20.861690521240234 Average Return = -500.000000\n",
      "step = 950: loss: 20.755489349365234 Average Return = -500.000000\n",
      "step = 1000: loss: 20.666915893554688 Average Return = -500.000000\n",
      "step = 1050: loss: 28.880447387695312 Average Return = -500.000000\n",
      "step = 1100: loss: 20.6431827545166 Average Return = -500.000000\n",
      "step = 1150: loss: 20.552490234375 Average Return = -500.000000\n",
      "step = 1200: loss: 20.50385284423828 Average Return = -500.000000\n",
      "step = 1250: loss: 20.491958618164062 Average Return = -500.000000\n",
      "step = 1300: loss: 20.488574981689453 Average Return = -500.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[237], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m step \u001B[38;5;241m=\u001B[39m agent_ppo\u001B[38;5;241m.\u001B[39mtrain_step_counter\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m eval_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 26\u001B[0m     avg_return \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_avg_return\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent_ppo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_eval_episodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep = \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m: loss: \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m Average Return = \u001B[39m\u001B[38;5;132;01m{2:2f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(step, train_loss,avg_return))\n\u001B[1;32m     28\u001B[0m     returns\u001B[38;5;241m.\u001B[39mappend(avg_return)\n",
      "Cell \u001B[0;32mIn[220], line 13\u001B[0m, in \u001B[0;36mcompute_avg_return\u001B[0;34m(environment, policy, num_episodes)\u001B[0m\n\u001B[1;32m     10\u001B[0m episode_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m time_step\u001B[38;5;241m.\u001B[39mis_last():\n\u001B[0;32m---> 13\u001B[0m     action_step \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_initial_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     time_step \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mstep(action_step\u001B[38;5;241m.\u001B[39maction)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# print('time step: {0}'.format(time_step))\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py:324\u001B[0m, in \u001B[0;36mTFPolicy.action\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_automatic_state_reset:\n\u001B[1;32m    323\u001B[0m   policy_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_reset_state(time_step, policy_state)\n\u001B[0;32m--> 324\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[43maction_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtime_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclip_action\u001B[39m(action, action_spec):\n\u001B[1;32m    327\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(action_spec, tensor_spec\u001B[38;5;241m.\u001B[39mBoundedTensorSpec):\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/utils/common.py:188\u001B[0m, in \u001B[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m check_tf1_allowed()\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_eager_been_enabled():\n\u001B[1;32m    186\u001B[0m   \u001B[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[39;00m\n\u001B[1;32m    187\u001B[0m   \u001B[38;5;66;03m# autodep-like behavior is already expected of fn.\u001B[39;00m\n\u001B[0;32m--> 188\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resource_variables_enabled():\n\u001B[1;32m    190\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py:560\u001B[0m, in \u001B[0;36mTFPolicy._action\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of `action`.\u001B[39;00m\n\u001B[1;32m    546\u001B[0m \n\u001B[1;32m    547\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;124;03m    `info`: Optional side information such as action log probabilities.\u001B[39;00m\n\u001B[1;32m    558\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    559\u001B[0m seed_stream \u001B[38;5;241m=\u001B[39m tfp\u001B[38;5;241m.\u001B[39mutil\u001B[38;5;241m.\u001B[39mSeedStream(seed\u001B[38;5;241m=\u001B[39mseed, salt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtf_agents_tf_policy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 560\u001B[0m distribution_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_state\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype: disable=wrong-arg-types\u001B[39;00m\n\u001B[1;32m    561\u001B[0m actions \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m d: reparameterized_sampling\u001B[38;5;241m.\u001B[39msample(d, seed\u001B[38;5;241m=\u001B[39mseed_stream()),\n\u001B[1;32m    563\u001B[0m     distribution_step\u001B[38;5;241m.\u001B[39maction)\n\u001B[1;32m    564\u001B[0m info \u001B[38;5;241m=\u001B[39m distribution_step\u001B[38;5;241m.\u001B[39minfo\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/policies/greedy_policy.py:80\u001B[0m, in \u001B[0;36mGreedyPolicy._distribution\u001B[0;34m(self, time_step, policy_state)\u001B[0m\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour network\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms distribution does not implement mode \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaking it incompatible with a greedy policy.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     76\u001B[0m                     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m     78\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tfp\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mDeterministic(loc\u001B[38;5;241m=\u001B[39mgreedy_action)\n\u001B[0;32m---> 80\u001B[0m distribution_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrapped_policy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribution\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m policy_step\u001B[38;5;241m.\u001B[39mPolicyStep(\n\u001B[1;32m     83\u001B[0m     tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(dist_fn, distribution_step\u001B[38;5;241m.\u001B[39maction),\n\u001B[1;32m     84\u001B[0m     distribution_step\u001B[38;5;241m.\u001B[39mstate, distribution_step\u001B[38;5;241m.\u001B[39minfo)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py:403\u001B[0m, in \u001B[0;36mTFPolicy.distribution\u001B[0;34m(self, time_step, policy_state)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_automatic_state_reset:\n\u001B[1;32m    402\u001B[0m   policy_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_reset_state(time_step, policy_state)\n\u001B[0;32m--> 403\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtime_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memit_log_probability:\n\u001B[1;32m    405\u001B[0m   \u001B[38;5;66;03m# This here is set only for compatibility with info_spec in constructor.\u001B[39;00m\n\u001B[1;32m    406\u001B[0m   info \u001B[38;5;241m=\u001B[39m policy_step\u001B[38;5;241m.\u001B[39mset_log_probability(\n\u001B[1;32m    407\u001B[0m       step\u001B[38;5;241m.\u001B[39minfo,\n\u001B[1;32m    408\u001B[0m       tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    409\u001B[0m           \u001B[38;5;28;01mlambda\u001B[39;00m _: tf\u001B[38;5;241m.\u001B[39mconstant(\u001B[38;5;241m0.\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32),\n\u001B[1;32m    410\u001B[0m           policy_step\u001B[38;5;241m.\u001B[39mget_log_probability(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_spec)))\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/agents/ppo/ppo_policy.py:249\u001B[0m, in \u001B[0;36mPPOPolicy._distribution\u001B[0;34m(self, time_step, policy_state, training)\u001B[0m\n\u001B[1;32m    244\u001B[0m   policy_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue_network_state\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m ()\n\u001B[1;32m    246\u001B[0m new_policy_state \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactor_network_state\u001B[39m\u001B[38;5;124m'\u001B[39m: (), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue_network_state\u001B[39m\u001B[38;5;124m'\u001B[39m: ()}\n\u001B[1;32m    248\u001B[0m (distributions, new_policy_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactor_network_state\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply_actor_network\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtime_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mactor_network_state\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_collect:\n\u001B[1;32m    253\u001B[0m   policy_info \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    254\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdist_params\u001B[39m\u001B[38;5;124m'\u001B[39m: ppo_utils\u001B[38;5;241m.\u001B[39mget_distribution_params(\n\u001B[1;32m    255\u001B[0m           distributions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    258\u001B[0m               network\u001B[38;5;241m.\u001B[39mDistributionNetwork))\n\u001B[1;32m    259\u001B[0m   }\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/agents/ppo/ppo_policy.py:222\u001B[0m, in \u001B[0;36mPPOPolicy._apply_actor_network\u001B[0;34m(self, time_step, policy_state, training)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_observation_normalizer:\n\u001B[1;32m    220\u001B[0m   observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_observation_normalizer\u001B[38;5;241m.\u001B[39mnormalize(observation)\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_actor_network\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtime_step\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/networks/network.py:427\u001B[0m, in \u001B[0;36mNetwork.__call__\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mis_tensor(network_state)\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m network_state \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, ())\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnetwork_state\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m call_argspec\u001B[38;5;241m.\u001B[39margs\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m call_argspec\u001B[38;5;241m.\u001B[39mkeywords):\n\u001B[1;32m    425\u001B[0m   normalized_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnetwork_state\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 427\u001B[0m outputs, new_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mNetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnormalized_kwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype: disable=attribute-error  # typed-keras\u001B[39;00m\n\u001B[1;32m    429\u001B[0m nest_utils\u001B[38;5;241m.\u001B[39massert_matching_dtypes_and_inner_shapes(\n\u001B[1;32m    430\u001B[0m     new_state,\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate_spec,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    434\u001B[0m     tensors_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`new_state`\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m     specs_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`state_spec`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs, new_state\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/engine/base_layer.py:1132\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_cast_inputs(inputs, input_list)\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m autocast_variable\u001B[38;5;241m.\u001B[39menable_auto_cast_variables(\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_dtype_object\n\u001B[1;32m   1131\u001B[0m ):\n\u001B[0;32m-> 1132\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activity_regularizer:\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001B[0m, in \u001B[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m bound_signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_keras_call_info_injected\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;66;03m# Only inject info for the innermost failing call\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/networks/actor_distribution_network.py:184\u001B[0m, in \u001B[0;36mActorDistributionNetwork.call\u001B[0;34m(self, observations, step_type, network_state, training, mask)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    179\u001B[0m          observations,\n\u001B[1;32m    180\u001B[0m          step_type,\n\u001B[1;32m    181\u001B[0m          network_state,\n\u001B[1;32m    182\u001B[0m          training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    183\u001B[0m          mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 184\u001B[0m   state, network_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m      \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m      \u001B[49m\u001B[43mstep_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstep_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnetwork_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m   outer_rank \u001B[38;5;241m=\u001B[39m nest_utils\u001B[38;5;241m.\u001B[39mget_outer_rank(observations, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_tensor_spec)\n\u001B[1;32m    191\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_projection_net\u001B[39m(proj_net):\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/networks/network.py:427\u001B[0m, in \u001B[0;36mNetwork.__call__\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mis_tensor(network_state)\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m network_state \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, ())\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnetwork_state\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m call_argspec\u001B[38;5;241m.\u001B[39margs\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m call_argspec\u001B[38;5;241m.\u001B[39mkeywords):\n\u001B[1;32m    425\u001B[0m   normalized_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnetwork_state\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 427\u001B[0m outputs, new_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mNetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnormalized_kwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype: disable=attribute-error  # typed-keras\u001B[39;00m\n\u001B[1;32m    429\u001B[0m nest_utils\u001B[38;5;241m.\u001B[39massert_matching_dtypes_and_inner_shapes(\n\u001B[1;32m    430\u001B[0m     new_state,\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate_spec,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    434\u001B[0m     tensors_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`new_state`\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m     specs_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`state_spec`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs, new_state\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/engine/base_layer.py:1132\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_cast_inputs(inputs, input_list)\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m autocast_variable\u001B[38;5;241m.\u001B[39menable_auto_cast_variables(\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_dtype_object\n\u001B[1;32m   1131\u001B[0m ):\n\u001B[0;32m-> 1132\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activity_regularizer:\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001B[0m, in \u001B[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m bound_signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_keras_call_info_injected\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;66;03m# Only inject info for the innermost failing call\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tf_agents/networks/encoding_network.py:328\u001B[0m, in \u001B[0;36mEncodingNetwork.call\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    325\u001B[0m   states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocessing_combiner(states)\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_postprocessing_layers:\n\u001B[0;32m--> 328\u001B[0m   states \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_squash:\n\u001B[1;32m    331\u001B[0m   states \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mmap_structure(batch_squash\u001B[38;5;241m.\u001B[39munflatten, states)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/engine/base_layer.py:1132\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_cast_inputs(inputs, input_list)\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m autocast_variable\u001B[38;5;241m.\u001B[39menable_auto_cast_variables(\n\u001B[1;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_dtype_object\n\u001B[1;32m   1131\u001B[0m ):\n\u001B[0;32m-> 1132\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_activity_regularizer:\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001B[0m, in \u001B[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m bound_signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_keras_call_info_injected\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;66;03m# Only inject info for the innermost failing call\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/keras/layers/core/dense.py:255\u001B[0m, in \u001B[0;36mDense.call\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    252\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mbias_add(outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactivation\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_ragged:\n\u001B[1;32m    258\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m original_inputs\u001B[38;5;241m.\u001B[39mwith_flat_values(outputs)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1170\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m iterable_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1169\u001B[0m   args, kwargs \u001B[38;5;241m=\u001B[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001B[0;32m-> 1170\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mapi_dispatcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1171\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[1;32m   1172\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "collect_steps_per_iteration = 10\n",
    "eval_interval = 10\n",
    "num_eval_episodes = 1\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent_ppo.train = common.function(agent_ppo.train)\n",
    "agent_ppo.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        # collect_step(train_env, agent_ppo.collect_policy)\n",
    "        collect_step(train_env, agent_ppo.collect_policy)\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    # print('Xp: {0}'.format(experience))\n",
    "    train_loss = agent_ppo.train(experience).loss\n",
    "\n",
    "    step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "        print('step = {0}: loss: {1} Average Return = {2:2f}'.format(step, train_loss,avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "(TimeStep(\n {'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n  'observation': <tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n array([[0.9973923 , 0.07217032, 0.9038277 , 0.42789653, 0.1573971 ,\n         0.23413408]], dtype=float32)>,\n  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n  'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>}),\n ())"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = 100\n",
    "\n",
    "driver = py_driver.PyDriver(\n",
    "    env=train_env,\n",
    "    policy=agent_ppo.collect_policy,\n",
    "    observers=[rb_observer],\n",
    "    max_steps=steps,\n",
    ")\n",
    "\n",
    "time_step = train_env.reset()\n",
    "driver.run(time_step)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[158], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Sample a batch of data from the buffer and update the agent's network.\u001B[39;00m\n\u001B[1;32m     19\u001B[0m experience, unused_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m---> 20\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43magent_ppo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# replay_buffer.clear()\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# print('Xp: {0}'.format(experience))\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# agent_ppo.train(experience)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m step \u001B[38;5;241m=\u001B[39m agent_ppo\u001B[38;5;241m.\u001B[39mtrain_step_counter\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Documents/pessoal/tg/git/learning-tg-agents/venvIntelliJ/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "collect_steps_per_iteration = 10\n",
    "num_eval_episodes = 1\n",
    "eval_interval = 10\n",
    "\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    train_env,\n",
    "    agent_ppo.collect_policy,\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps and save to the replay buffer.\n",
    "    # time_step, _ = collect_driver.run(time_step)\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent_ppo.train(experience).loss\n",
    "    # replay_buffer.clear()\n",
    "    # print('Xp: {0}'.format(experience))\n",
    "    # agent_ppo.train(experience)\n",
    "    step = agent_ppo.train_step_counter.numpy()\n",
    "\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent_ppo.policy, num_eval_episodes)\n",
    "        print('step = {0}; Average Return= {1}; loss = {2}'.format(step, avg_return,train_loss))\n",
    "        # print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        if(agent_ppo.collect_policy == agent_ppo.policy):\n",
    "            print(\"True\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, 2)\n",
    "# steps = range(0, num_iterations + 1, 10)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd2ee761d2d7da4166c50099226d582fd9408a55bb0c94aecff2a6acb1ce196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
